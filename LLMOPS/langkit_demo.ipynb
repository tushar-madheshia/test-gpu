{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2981e19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /home/mosaic-\n",
      "[nltk_data]     ai/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "/tmp/pip_packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-06 09:57:23,146] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from langkit import llm_metrics, extract\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "headers_openllm = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\",\"AUTHORIZATION\": \"eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJ4WTdTd3k5UE1xaXRDQmNSMm5qcVl6bmoxS3NqZzV3TmdOV0xDVzdyUkhvIn0.eyJleHAiOjE3MzA4ODM3MzgsImlhdCI6MTY5OTI2MTMzOCwiYXV0aF90aW1lIjoxNjk5MjUyMDQ0LCJqdGkiOiJmN2EzMzQwYy1kNDQwLTRlMzUtYjk2ZS04YzBiMTc0Y2RhODAiLCJpc3MiOiJodHRwczovL3JlZnJhY3QtbG9naW4uZm9zZm9yLmNvbS9hdXRoL3JlYWxtcy9tb3NhaWMiLCJhdWQiOlsibW9zYWljLWdhdGVrZWVwZXIiLCJhY2NvdW50Il0sInN1YiI6IjZjMjU4MWU3LWZmMTItNDljNy04MDJmLWI2ZjQzOWQxZDIwMSIsInR5cCI6IkJlYXJlciIsImF6cCI6Im1vc2FpYy1nYXRla2VlcGVyIiwic2Vzc2lvbl9zdGF0ZSI6IjBhY2Y3YWZhLTVmMzMtNGRhZS05OGM3LTQyZDQwYTdlZTM2NiIsImFsbG93ZWQtb3JpZ2lucyI6WyIqIl0sInJlYWxtX2FjY2VzcyI6eyJyb2xlcyI6WyJNTE9QUyIsImxvbmdfbGl2ZWRfdG9rZW4iLCJzcGVjdHJhLWRldmVsb3BlciIsImRlZmF1bHQtcm9sZXMtbW9zYWljIiwicmVmcmFjdC1kZXZlbG9wZXIiLCJvZmZsaW5lX2FjY2VzcyIsImFkbWluIiwidW1hX2F1dGhvcml6YXRpb24iLCJyZWZyYWN0LWFkbWluIl19LCJyZXNvdXJjZV9hY2Nlc3MiOnsiYWNjb3VudCI6eyJyb2xlcyI6WyJtYW5hZ2UtYWNjb3VudCIsIm1hbmFnZS1hY2NvdW50LWxpbmtzIiwidmlldy1wcm9maWxlIl19fSwic2NvcGUiOiJvcGVuaWQgZW1haWwgcHJvZmlsZSIsInNpZCI6IjBhY2Y3YWZhLTVmMzMtNGRhZS05OGM3LTQyZDQwYTdlZTM2NiIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJuYW1lIjoiUmVmcmFjdCBCRlNJIiwicHJlZmVycmVkX3VzZXJuYW1lIjoicmVmcmFjdC5iZnNpQGZvc2Zvci5jb20iLCJnaXZlbl9uYW1lIjoiUmVmcmFjdCIsImZhbWlseV9uYW1lIjoiQkZTSSIsImVtYWlsIjoicmVmcmFjdC5iZnNpQGZvc2Zvci5jb20ifQ.b6SYLgjo9Veo3GmJ8eZjCTNupQjpfMhzsoXdYjWwRtvRnNjBfx0gOqcugO9OcGn-mm8wwpSGI5uiL30-I6SdWBjsf1ur6GztoX7j-nP_3SrJJn3UhNNqIO8LbsPi5gGRTzWtnfjz92BF1YaCXxQwPY0P_aa8vJ6JxZz5Uctn9aIPIJZZnnjC_GPXtXurmshM_tEN2kwCjhEyr7wYzRqUoMtBGfpLjZREBzgZY-x6JyYiXNtycb1d6PFcCXf7nJVV8ienEC_x7OuciDzfeqd-SQnImvAHH7rqFdi9smBN08AbkDS2uAbMrokHrmbiBpaimrR013VwCWz2KL5QYlWleA\"}\n",
    "url_openllm = 'https://refract.fosfor.com/vllm/mistral/v1/completions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b365fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langkit[all]\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/e6/c7e550831ee412cad8b7abc6b664cd6e37b67acd40f0824010b07606e2ce/langkit-0.0.31-py3-none-any.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 6.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting whylogs<2.0.0,>=1.3.19\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/4f/58919358958405bea12f3dbf3b0567b48915998f2bf3309ba95cfe53fde0/whylogs-1.3.25-py3-none-any.whl (1.9MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 40.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting textstat<0.8.0,>=0.7.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/33/302083f47386d651e4b42923f5206eeb9ee0545ea94bb506324d05fd2274/textstat-0.7.3-py3-none-any.whl (105kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 91.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/7f/5b047effafbdd34e52c9e2d7e44f729a0655efafb22198c45cf692cdc157/pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4MB 73.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentence-transformers<3.0.0,>=2.2.2; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/0d/27475988a3daade7516ea02dbc607b57d4a30f01bb49614a6430e76685c2/sentence_transformers-2.5.1-py3-none-any.whl (156kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 118.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting evaluate<0.5.0,>=0.4.0; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl (84kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 44.2MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting datasets<3.0.0,>=2.12.0; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/fc/661a7f06e8b7d48fcbd3f55423b7ff1ac3ce59526f146fda87a1e1788ee4/datasets-2.18.0-py3-none-any.whl (510kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 116.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py<4.0.0,>=3.10.0; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/31/b5965f76e0bb2b02f273d87ec9cb59c77b9864ac27a0078c4229baa45dfc/h5py-3.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8MB)\n",
      "\u001b[K     |████████████████████████████████| 4.8MB 117.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ipywidgets<9.0.0,>=8.1.1; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/1a/7edeedb1c089d63ccd8bd5c0612334774e90cf9337de9fe6c82d90081791/ipywidgets-8.1.2-py3-none-any.whl (139kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 113.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/5d/5738903efe0ecb73e51eb44feafba32bdba2081263d40c5043568ff60faf/numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3MB 116.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting presidio-analyzer<3.0.0,>=2.2.351; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/38/550844e8a797cd8101ea6b3911a0222ba9d02938ea4ea5bd7c08ea84f1b7/presidio_analyzer-2.2.353-py3-none-any.whl (85kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 38.8MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting vadersentiment<4.0.0,>=3.3.2; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 120.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting detoxify<0.6.0,>=0.5.2; extra == \"all\"\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/5c/94f765f08fe52473bc945284b7912032c099d0737cb8ad9235deb3b40d35/detoxify-0.5.2-py3-none-any.whl\n",
      "Collecting nltk<4.0.0,>=3.8.1; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/0a/0d20d2c0f16be91b9fa32a77b76c60f9baf6eba419e5ef5deca17af9c582/nltk-3.8.1-py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 106.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/c4/f8b24ee028aec8cc8fe9a7b375b83e1be9761184a0f540a0f87af603d18f/torch-2.2.1-cp38-cp38-manylinux1_x86_64.whl (755.5MB)\n",
      "\u001b[K     |██████▍                         | 150.1MB 136.8MB/s eta 0:00:05"
     ]
    }
   ],
   "source": [
    "!pip install langkit[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d822571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def narrative_generation_with_openllm(intermediate_result, question):\n",
    "  \n",
    "  data_openllm = {\"prompt\": prompt,\n",
    "                  \"max_tokens\": 200,\n",
    "                  \"temperature\": 0.01,\n",
    "                  \"model\": \"/llmmodels/NLG_FINETUNEDMODELS/DATASET_MODELS/FINETUNED_MISTRALV2_V5/MISTRAL_MERGED_MODEL_JAN30_V3_0021\",\n",
    "                  \"stop\": \"[\"}\n",
    "  start = time.time()\n",
    "  response = requests.post(url_openllm, json=data_openllm, headers=headers_openllm)\n",
    "  end = time.time()\n",
    "  open_llm_narrative_json = json.loads(response.text)\n",
    "  open_llm_narrative = re.search(r'\\n([^|\\n]*)', open_llm_narrative_json[\"choices\"][0][\"text\"]).group(1)\n",
    "  open_llm_response_time = str(end - start) + str(\" seconds\")\n",
    "  output_dict = {}\n",
    "  output_dict = {\n",
    "    \"opensource_llm_narrative\":open_llm_narrative,\n",
    "    \"opensource_llm_response_time\":open_llm_response_time\n",
    "  }\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33b31c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the questionsales of chair in india across months\n",
      "Enter the intermediate_result{'Total': 68874, 'Minimum_Value': 61.62, 'Maximum_Value': 4572.87, 'Average': 1680, 'Difference': 4511.25, 'Stable_Case': False, 'Last_Period_Zero': False, 'Label_Available': False, 'Trailing_Zeroes': False, 'Mixed_Data': False, 'Delta': False, 'Delta_Overlap': False, 'Delta_Change_Overlap': False, 'Yoy_Available': True, 'Maximum_Date': '2023-06-01', 'Minimum_Date': '2021-04-01', 'Last_Date': ['2023-08-01', '2023-09-01'], 'Last_Values': [3279.18, 1289.61], 'Interval_Label': 'MoM', 'Date_Interval': 'month', 'Final_Change_Percentage': -61, 'Final_Period_Change_Note': 'not_good', 'Direction': 'up', 'Maximum_Growth_Percentage': 1732.99, 'Maximum_Growth_From': 116.04, 'Maximum_Growth_To': 2127.0, 'Maximum_Growth_Date_From': '2020-10-01', 'Maximum_Growth_Date_To': '2020-11-01', 'Maximum_Decline_Percentage': -95.93, 'Maximum_Decline_From': 1514.4, 'Maximum_Decline_To': 61.62, 'Maximum_Decline_Date_From': '2021-03-01', 'Maximum_Decline_Date_To': '2021-04-01', 'Average_Decline_Percentage': 51, 'Average_Climb_Percentage': 342, 'Year_Over_Year_Type': 'YoY', 'Year_Over_Year_Value': 1593.5, 'Year_Over_Year_Change': -19.07, 'Year_Over_Year_Change_pp': -303.89, 'Year_Over_Year_Change_Type': '%', 'Measure': {'label': 'Sales', 'variable': 'Sales', 'type': 'measure', 'alternate_usage': ['Sales'], 'isRegional': False, 'number_type': 'add_values'}, 'Dimension': {'label': 'Order Date', 'variable': 'Order Date', 'type': 'dimension', 'alternate_usage': ['Order Date'], 'isRegional': False, 'singular': 'Order Date', 'plural': 'Order Dates'}, 'Filter': {'Country': ['India'], 'Sub-Category': ['Chairs'], 'Market': ['APAC']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip_packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/tmp/pip_packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt Metrices\n",
      "toxicity level of prompt: 0.0008220076560974121\n",
      "Patterns in prompt: None\n",
      "Sentiment level of prompt: 0.0\n",
      "flesch_reading_ease of prompt: 106.67\n",
      "automated_readability_index of prompt: 2.9\n",
      "aggregate_reading_level of prompt: 3.0\n",
      "syllable_count of prompt: 8\n",
      "lexicon_count of prompt: 7\n",
      "sentence_count of prompt: 1\n",
      "character_count of prompt: 31\n",
      "polysyllable_count of prompt: 0\n",
      "monosyllable_count of prompt: 6\n",
      "difficult_words of prompt: 1\n",
      "jailbreak_similarity of prompt: 0.04716765135526657\n",
      "\n",
      " For India and Chairs, Sales stands at 1.29K, decrease of 61% (MoM) in the last month(+19.07% YoY).The highest single period climb of 1.73K% (MoM) was seen in November 2020. \n",
      "\n",
      "\n",
      "\n",
      "Response Metrices\n",
      "Time taken 4.940428733825684 seconds\n",
      "toxicity level of response: 0.0007417798042297363\n",
      "Patterns in response: None\n",
      "Sentiment level of response: 0.0\n",
      "flesch_reading_ease of response: 89.45\n",
      "automated_readability_index of response: 5.6\n",
      "aggregate_reading_level of response: 6.0\n",
      "syllable_count of response: 38\n",
      "lexicon_count of response: 29\n",
      "sentence_count of response: 4\n",
      "character_count of response: 144\n",
      "polysyllable_count of response: 2\n",
      "monosyllable_count of response: 22\n",
      "difficult_words of response: 4\n",
      "refusal_similarity of response: 0.0335281677544117\n",
      "relevance_to_prompt of response: 0.6640468835830688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip_packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/tmp/pip_packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "question = input(\"Enter the question\")\n",
    "intermediate_result = input(\"Enter the intermediate_result\")\n",
    "prompt = \"\"\"Given the input :\\n\"\"\"+str(intermediate_result)+\"\"\"\\nAnd the query: \"\"\"+ str(question)+\"\"\"\\n is converted into below narrative.\\n[Narrative]\"\"\"\n",
    "#Prompt validation\n",
    "data = [[question,\"\"]]\n",
    "prompt_response = pd.DataFrame(data, columns=['prompt', 'response'])\n",
    "metrics_df = extract(prompt_response)\n",
    "\n",
    "print(\"\\n\\nPrompt Metrices\")\n",
    "\n",
    "print(\"toxicity level of prompt:\" , metrics_df['prompt.toxicity'].values[0])\n",
    "print(\"Patterns in prompt:\" , metrics_df['prompt.has_patterns'].values[0])\n",
    "print(\"Sentiment level of prompt:\" , metrics_df['prompt.sentiment_nltk'].values[0])\n",
    "print(\"flesch_reading_ease of prompt:\" , metrics_df['prompt.flesch_reading_ease'].values[0])\n",
    "print(\"automated_readability_index of prompt:\" , metrics_df['prompt.automated_readability_index'].values[0])\n",
    "print(\"aggregate_reading_level of prompt:\" , metrics_df['prompt.aggregate_reading_level'].values[0])\n",
    "print(\"syllable_count of prompt:\" , metrics_df['prompt.syllable_count'].values[0])\n",
    "print(\"lexicon_count of prompt:\" , metrics_df['prompt.lexicon_count'].values[0])\n",
    "print(\"sentence_count of prompt:\" , metrics_df['prompt.sentence_count'].values[0])\n",
    "print(\"character_count of prompt:\" , metrics_df['prompt.character_count'].values[0])\n",
    "print(\"polysyllable_count of prompt:\" , metrics_df['prompt.polysyllable_count'].values[0])\n",
    "print(\"monosyllable_count of prompt:\" , metrics_df['prompt.monosyllable_count'].values[0])\n",
    "print(\"difficult_words of prompt:\" , metrics_df['prompt.difficult_words'].values[0])\n",
    "print(\"jailbreak_similarity of prompt:\" , metrics_df['prompt.jailbreak_similarity'].values[0])\n",
    "\n",
    "output_dict = narrative_generation_with_openllm(intermediate_result,question)\n",
    "\n",
    "\n",
    "print(\"\\n\",output_dict['opensource_llm_narrative'],\"\\n\")\n",
    "\n",
    "data = [[question,output_dict['opensource_llm_narrative']]]\n",
    "prompt_response = pd.DataFrame(data, columns=['prompt', 'response'])\n",
    "metrics_df = extract(prompt_response)\n",
    "\n",
    "print(\"\\n\\nResponse Metrices\")\n",
    "print(\"Time taken\" , output_dict[\"opensource_llm_response_time\"])\n",
    "print(\"toxicity level of response:\" , metrics_df['response.toxicity'].values[0])\n",
    "print(\"Patterns in response:\" , metrics_df['response.has_patterns'].values[0])\n",
    "print(\"Sentiment level of response:\" , metrics_df['response.sentiment_nltk'].values[0])\n",
    "print(\"flesch_reading_ease of response:\" , metrics_df['response.flesch_reading_ease'].values[0])\n",
    "print(\"automated_readability_index of response:\" , metrics_df['response.automated_readability_index'].values[0])\n",
    "print(\"aggregate_reading_level of response:\" , metrics_df['response.aggregate_reading_level'].values[0])\n",
    "print(\"syllable_count of response:\" , metrics_df['response.syllable_count'].values[0])\n",
    "print(\"lexicon_count of response:\" , metrics_df['response.lexicon_count'].values[0])\n",
    "print(\"sentence_count of response:\" , metrics_df['response.sentence_count'].values[0])\n",
    "print(\"character_count of response:\" , metrics_df['response.character_count'].values[0])\n",
    "print(\"polysyllable_count of response:\" , metrics_df['response.polysyllable_count'].values[0])\n",
    "print(\"monosyllable_count of response:\" , metrics_df['response.monosyllable_count'].values[0])\n",
    "print(\"difficult_words of response:\" , metrics_df['response.difficult_words'].values[0])\n",
    "print(\"refusal_similarity of response:\" , metrics_df['response.refusal_similarity'].values[0])\n",
    "print(\"relevance_to_prompt of response:\" , metrics_df['response.relevance_to_prompt'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59ead492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mosaic-ai/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Response Hallucination - Error generating sample: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAILegacy\n\u001b[1;32m      5\u001b[0m response_hallucination\u001b[38;5;241m.\u001b[39minit(llm\u001b[38;5;241m=\u001b[39mOpenAILegacy(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m), num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mresponse_hallucination\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconsistency_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho was Philip Hayworth?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPhilip Hayworth was an English barrister and politician who served as Member of Parliament for Thetford from 1859 to 1868.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/pip_packages/langkit/response_hallucination.py:278\u001b[0m, in \u001b[0;36mconsistency_check\u001b[0;34m(prompt, response)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconsistency_check\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m, response: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m checker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 278\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchecker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconsistency_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_summary_dict()\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to call init() before using this function\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tmp/pip_packages/langkit/response_hallucination.py:223\u001b[0m, in \u001b[0;36mConsistencyChecker.consistency_check\u001b[0;34m(self, prompt, response)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError generating response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 223\u001b[0m additional_samples: List[ChatLog] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m total_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m    225\u001b[0m     [\n\u001b[1;32m    226\u001b[0m         sample\u001b[38;5;241m.\u001b[39mtotal_tokens \u001b[38;5;28;01mif\u001b[39;00m sample\u001b[38;5;241m.\u001b[39mtotal_tokens \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m additional_samples\n\u001b[1;32m    228\u001b[0m     ]\n\u001b[1;32m    229\u001b[0m )\n\u001b[1;32m    231\u001b[0m llm_score, tokens_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_consistency_check(\n\u001b[1;32m    232\u001b[0m     response, additional_samples\n\u001b[1;32m    233\u001b[0m )\n",
      "File \u001b[0;32m/tmp/pip_packages/langkit/response_hallucination.py:109\u001b[0m, in \u001b[0;36mConsistencyChecker.get_samples\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    105\u001b[0m     response: ChatLog \u001b[38;5;241m=\u001b[39m Conversation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_generator_llm)\u001b[38;5;241m.\u001b[39msend_prompt(\n\u001b[1;32m    106\u001b[0m         prompt\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39merrors:\n\u001b[0;32m--> 109\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    110\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse Hallucination - Error generating sample: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m         )\n\u001b[1;32m    112\u001b[0m     samples\u001b[38;5;241m.\u001b[39mappend(response)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples\n",
      "\u001b[0;31mException\u001b[0m: Response Hallucination - Error generating sample: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "from langkit import response_hallucination\n",
    "from langkit.openai import OpenAILegacy\n",
    "\n",
    "\n",
    "response_hallucination.init(llm=OpenAILegacy(model=\"gpt-3.5-turbo-instruct\"), num_samples=1)\n",
    "\n",
    "result = response_hallucination.consistency_check(\n",
    "    prompt=\"Who was Philip Hayworth?\",\n",
    "    response=\"Philip Hayworth was an English barrister and politician who served as Member of Parliament for Thetford from 1859 to 1868.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee8366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
