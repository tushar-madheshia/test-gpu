{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c3136d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://lumin-azure-openai-rnd.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_KEY\"] = \"38002279fa574fbcba0fbc424ae056d5\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"38002279fa574fbcba0fbc424ae056d5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed770a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "colang_content = \"\"\"\n",
    "\n",
    "# define niceties\n",
    "define user express greeting\n",
    "    \"hello\"\n",
    "    \"hi\"\n",
    "    \"what's up?\"\n",
    "\n",
    "define flow greeting\n",
    "    user express greeting\n",
    "    bot express greeting\n",
    "    bot ask how are you\n",
    "\n",
    "# define limits\n",
    "define user ask politics\n",
    "    \"what are your political beliefs?\"\n",
    "    \"thoughts on the president?\"\n",
    "    \"left wing\"\n",
    "    \"right wing\"\n",
    "\n",
    "define bot answer politics\n",
    "    \"I'm a shopping assistant, I don't like to talk of politics.\"\n",
    "\n",
    "define flow politics\n",
    "    user ask politics\n",
    "    bot answer politics\n",
    "    bot offer help\n",
    "\"\"\"\n",
    "yaml_content = \"\"\"\n",
    "type: main\n",
    "engine: azure\n",
    "model: lumin-rnd-gpt-4-32k\n",
    "parameters:\n",
    "    azure_endpoint: https://lumin-azure-openai-rnd.openai.azure.com/\n",
    "    api_version: 2023-03-15-preview\n",
    "    deployment_name: lumin-rnd-gpt-4-32k\n",
    "    api_key: 38002279fa574fbcba0fbc424ae056d5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1dacb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip_packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772998fe2fe1456198f53682c4743c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6b3ff8f99848fabd1cdc29a12fe36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706f7cdca3724d8e809cb1447e96fbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ecd5a032064fa59464101d1e49125d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27745adadc24fbf967d738d74b81447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f225c068a41e40cea9ccc66717c1555b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81aa512c3fd64fb9ac8e3d23da5f2d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f60e5f9ff3e461a9635a0cb07908db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c4b6c0c899404ebdb6bfa87e69c4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e557498415940f4b1de869790561606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5ca30e5acc4374a8adb26f962a93eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b41eea845a3485591dcdd7f064e4f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ad75553d58420d98648f59df8fef1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de88dd36effd4f25997639bb31963344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip_packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "config = RailsConfig.from_content(\n",
    "    yaml_content=yaml_content,\n",
    "    colang_content=colang_content\n",
    ")\n",
    "rails = LLMRails(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "494704cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for NoneType\n",
      "Error 'NoneType' object has no attribute 'agenerate_prompt' while execution generate_user_intent\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/pip_packages/nemoguardrails/actions/action_dispatcher.py\", line 125, in execute_action\n",
      "    result = await fn(**params)\n",
      "  File \"/tmp/pip_packages/nemoguardrails/actions/llm/generation.py\", line 270, in generate_user_intent\n",
      "    result = await llm_call(llm, prompt)\n",
      "  File \"/tmp/pip_packages/nemoguardrails/actions/llm/utils.py\", line 31, in llm_call\n",
      "    result = await llm.agenerate_prompt(\n",
      "AttributeError: 'NoneType' object has no attribute 'agenerate_prompt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, an internal error has occurred.\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"Hey there!\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a95ff05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Collecting openai==1.13.3\n",
      "  Using cached openai-1.13.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai==1.13.3)\n",
      "  Using cached anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai==1.13.3)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai==1.13.3)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.13.3)\n",
      "  Using cached pydantic-2.6.4-py3-none-any.whl.metadata (85 kB)\n",
      "Collecting sniffio (from openai==1.13.3)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai==1.13.3)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions<5,>=4.7 (from openai==1.13.3)\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai==1.13.3)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.5.0->openai==1.13.3)\n",
      "  Using cached exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai==1.13.3)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.13.3)\n",
      "  Using cached httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.13.3)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai==1.13.3)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic<3,>=1.9.0->openai==1.13.3)\n",
      "  Using cached pydantic_core-2.16.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Using cached openai-1.13.3-py3-none-any.whl (227 kB)\n",
      "Using cached anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "Using cached pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
      "Using cached pydantic_core-2.16.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: typing-extensions, tqdm, sniffio, idna, h11, exceptiongroup, distro, certifi, annotated-types, pydantic-core, httpcore, anyio, pydantic, httpx, openai\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastapi 0.96.0 requires pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2, but you have pydantic 2.6.4 which is incompatible.\n",
      "langchain 0.0.251 requires pydantic<2,>=1, but you have pydantic 2.6.4 which is incompatible.\n",
      "langchain-community 0.0.28 requires langsmith<0.2.0,>=0.1.0, but you have langsmith 0.0.92 which is incompatible.\n",
      "langchain-core 0.1.31 requires langsmith<0.2.0,>=0.1.0, but you have langsmith 0.0.92 which is incompatible.\n",
      "nemoguardrails 0.4.0 requires httpx==0.23.3, but you have httpx 0.27.0 which is incompatible.\n",
      "nemoguardrails 0.4.0 requires Jinja2==3.1.2, but you have jinja2 3.1.3 which is incompatible.\n",
      "nemoguardrails 0.4.0 requires pydantic~=1.10.6, but you have pydantic 2.6.4 which is incompatible.\n",
      "nemoguardrails 0.4.0 requires typing-extensions==4.5.0, but you have typing-extensions 4.10.0 which is incompatible.\n",
      "jupyterlab 3.2.4 requires jupyter-server~=1.4, but you have jupyter-server 2.7.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.6.0 anyio-4.3.0 certifi-2024.2.2 distro-1.9.0 exceptiongroup-1.2.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 idna-3.6 openai-1.13.3 pydantic-2.6.4 pydantic-core-2.16.3 sniffio-1.3.1 tqdm-4.66.2 typing-extensions-4.10.0\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/sniffio-1.3.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/typing_extensions.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/distro already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/httpx already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/annotated_types already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/h11 already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/typing_extensions-4.10.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/annotated_types-0.6.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/distro-1.9.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/anyio-4.3.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/sniffio already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/certifi-2024.2.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/h11-0.14.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pydantic_core-2.16.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/httpcore already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/httpx-0.27.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/tqdm-4.66.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/openai-1.13.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/idna-3.6.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/idna already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/exceptiongroup already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pydantic-2.6.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/tqdm already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pydantic_core already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/openai already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/exceptiongroup-1.2.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/httpcore-1.0.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pydantic already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/anyio already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openai==1.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f776dce",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'AzureOpenAI'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[0;32m---> 10\u001b[0m AZURE_LLM \u001b[38;5;241m=\u001b[39m \u001b[43mAzureChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mazure_endpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOPENAI_API_BASE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenai_api_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOPENAI_API_VERSION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeployment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEPLOYMENT_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenai_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOPENAI_API_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenai_api_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOPENAI_API_TYPE\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/tmp/pip_packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/tmp/pip_packages/pydantic/main.py:1100\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain_openai/chat_models/azure.py:190\u001b[0m, in \u001b[0;36mAzureChatOpenAI.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    160\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAs of openai>=1.0.0, if `azure_deployment` (or alias \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`deployment_name`) is specified then \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_url=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://xxx.openai.azure.com/openai/deployments/my-deployment\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    170\u001b[0m         )\n\u001b[1;32m    171\u001b[0m client_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m\"\u001b[39m: values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_api_version\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure_endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure_endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    189\u001b[0m }\n\u001b[0;32m--> 190\u001b[0m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAzureOpenAI\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params)\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[1;32m    191\u001b[0m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync_client\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mAsyncAzureOpenAI(\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params\n\u001b[1;32m    193\u001b[0m )\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'AzureOpenAI'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "OPENAI_API_BASE = \"https://lumin-azure-openai-rnd.openai.azure.com/\"\n",
    "OPENAI_API_VERSION = \"2023-03-15-preview\"\n",
    "DEPLOYMENT_NAME = 'lumin-rnd-gpt-4-32k'\n",
    "OPENAI_API_KEY =\"38002279fa574fbcba0fbc424ae056d5\"\n",
    "OPENAI_API_TYPE=\"azure\"\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "AZURE_LLM = AzureChatOpenAI(\n",
    "    azure_endpoint=OPENAI_API_BASE,\n",
    "    openai_api_version=OPENAI_API_VERSION,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_type=OPENAI_API_TYPE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0347f84b",
   "metadata": {},
   "source": [
    "# Without guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fce29a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Got unknown type Explain step by step. How old is the president of USA?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[1;32m      2\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain step by step. How old is the president of USA?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mchat_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain/chat_models/base.py:538\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    533\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    537\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 538\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain/chat_models/base.py:296\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    295\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    297\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    298\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    300\u001b[0m ]\n\u001b[1;32m    301\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain/chat_models/base.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 286\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m         )\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain/chat_models/base.py:433\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain/chat_models/openai.py:398\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m generation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[generation])\n\u001b[0;32m--> 398\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_message_dicts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    400\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_with_retry(\n\u001b[1;32m    401\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    402\u001b[0m )\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain/chat_models/openai.py:413\u001b[0m, in \u001b[0;36mChatOpenAI._create_message_dicts\u001b[0;34m(self, messages, stop)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`stop` found in both the input and default params.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    412\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[0;32m--> 413\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m [_convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message_dicts, params\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain/chat_models/openai.py:413\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`stop` found in both the input and default params.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    412\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[0;32m--> 413\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m [\u001b[43m_convert_message_to_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message_dicts, params\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain/chat_models/openai.py:175\u001b[0m, in \u001b[0;36m_convert_message_to_dict\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    169\u001b[0m     message_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    173\u001b[0m     }\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unknown type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39madditional_kwargs:\n\u001b[1;32m    177\u001b[0m     message_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39madditional_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Got unknown type Explain step by step. How old is the president of USA?"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "msg = \"Explain step by step. How old is the president of USA?\"\n",
    "print(chat_model(messages=[msg]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ddc497",
   "metadata": {},
   "outputs": [],
   "source": [
    "colang_content = \"\"\"\n",
    "\n",
    "# define niceties\n",
    "define user express greeting\n",
    "    \"hello\"\n",
    "    \"hi\"\n",
    "    \"what's up?\"\n",
    "\n",
    "define flow greeting\n",
    "    user express greeting\n",
    "    bot express greeting\n",
    "    bot ask how are you\n",
    "\n",
    "# define limits\n",
    "define user ask politics\n",
    "    \"what are your political beliefs?\"\n",
    "    \"thoughts on the president?\"\n",
    "    \"left wing\"\n",
    "    \"right wing\"\n",
    "\n",
    "define bot answer politics\n",
    "    \"I'm a shopping assistant, I don't like to talk of politics.\"\n",
    "\n",
    "define flow politics\n",
    "    user ask politics\n",
    "    bot answer politics\n",
    "    bot offer help\n",
    "\"\"\"\n",
    "yaml_content = \"\"\"\n",
    "type: main\n",
    "engine: azure\n",
    "model: lumin-rnd-gpt-4-32k\n",
    "parameters:\n",
    "    azure_endpoint: https://lumin-azure-openai-rnd.openai.azure.com/\n",
    "    api_version: 2023-03-15-preview\n",
    "    deployment_name: lumin-rnd-gpt-4-32k\n",
    "    api_key: 38002279fa574fbcba0fbc424ae056d5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc297b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip_packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/tmp/pip_packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "config = RailsConfig.from_content(\n",
    "    yaml_content=yaml_content,\n",
    "    colang_content=colang_content\n",
    ")\n",
    "app = LLMRails(config,llm=chat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf44dd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI assistant, I am neutral and do not have personal opinions on political matters. However, I can provide you with information about the president or any policies if you'd like.\n"
     ]
    }
   ],
   "source": [
    "# sample user input\n",
    "# new_message = app.generate(messages=[{\n",
    "#     \"role\": \"user\",\n",
    "#     \"content\": \"what's up?\"\n",
    "# }])\n",
    "\n",
    "# print(f\"new_message: {new_message}\")\n",
    "\n",
    "res = await app.generate_async(prompt=\"what do you think of the president?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0efc53e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Got unknown type content='Explain step by step. How old is the president of USA?'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[1;32m      2\u001b[0m msg \u001b[38;5;241m=\u001b[39m HumanMessage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain step by step. How old is the president of USA?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mchat_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain/chat_models/base.py:538\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    533\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    537\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 538\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain/chat_models/base.py:296\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    295\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    297\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    298\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    300\u001b[0m ]\n\u001b[1;32m    301\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain/chat_models/base.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 286\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m         )\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain/chat_models/base.py:433\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain/chat_models/openai.py:398\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m generation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[generation])\n\u001b[0;32m--> 398\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_message_dicts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    400\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_with_retry(\n\u001b[1;32m    401\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    402\u001b[0m )\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain/chat_models/openai.py:413\u001b[0m, in \u001b[0;36mChatOpenAI._create_message_dicts\u001b[0;34m(self, messages, stop)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`stop` found in both the input and default params.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    412\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[0;32m--> 413\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m [_convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message_dicts, params\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain/chat_models/openai.py:413\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`stop` found in both the input and default params.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    412\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[0;32m--> 413\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m [\u001b[43m_convert_message_to_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message_dicts, params\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain/chat_models/openai.py:175\u001b[0m, in \u001b[0;36m_convert_message_to_dict\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    169\u001b[0m     message_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    173\u001b[0m     }\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unknown type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39madditional_kwargs:\n\u001b[1;32m    177\u001b[0m     message_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39madditional_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Got unknown type content='Explain step by step. How old is the president of USA?'"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "msg = HumanMessage(\"Explain step by step. How old is the president of USA?\")\n",
    "print(chat_model(messages=[msg]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e356b447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
