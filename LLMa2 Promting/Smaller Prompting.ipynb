{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95cf623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/bd/f64d67df4d3b05a460f281defe830ffab6d7940b7ca98ec085e94e024781/transformers-4.34.1-py3-none-any.whl (7.7MB)\n",
      "\u001b[K     |████████████████████████████████| 7.8MB 6.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.21.1)\n",
      "Requirement already satisfied, skipping upgrade: safetensors>=0.3.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.3.2)\n",
      "Collecting tokenizers<0.15,>=0.14\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/ee/7e35fb46c728989357e6ccb96df64c4364601cfbfdd6c25ccc872e6c16a0/tokenizers-0.14.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 86.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied, skipping upgrade: fsspec in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.1.1)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Found existing installation: transformers 4.31.0\n",
      "    Uninstalling transformers-4.31.0:\n",
      "      Successfully uninstalled transformers-4.31.0\n",
      "Successfully installed tokenizers-0.14.1 transformers-4.34.1\n"
     ]
    }
   ],
   "source": [
    "#!sudo pip install --upgrade transformers\n",
    "#!sudo pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b08b1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.34.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig\n",
    "import json\n",
    "#from Gpt4_OpenAI_Prompt import system_msg,context,postprocess\n",
    "#from prompt1 import system_msg\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from fuzzywuzzy import process\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64bee884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "#print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0da20327",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = {\n",
    "    \"MEASURE\": [{\"ENTITY\": \"Discount\", \"other names\": [\"discount\", \"discount rate\", \"discount value\", \"deduction\"]},\n",
    "                {\"ENTITY\": \"Purchase Vol\", \"other names\": [\"purchase\", \"purchase value\", \"purchase model\"]},\n",
    "                {\"ENTITY\": \"Quantity\", \"other names\": [\"quantity\", \"volume\"]},\n",
    "                {\"ENTITY\": \"Sales\", \"other names\": [\"sales\", \"sale\"]}],\n",
    "    \"DIMENSION\": [{\"ENTITY\": \"Sub-Category\", \"other names\": [\"sub-category\", \"sub category\", \"categories\", \"section\"]},\n",
    "                  {\"ENTITY\": \"Segment\", \"other names\": [\"segment\", \"segments\", \"units\", \"divisions\"]},\n",
    "                  {\"ENTITY\": \"Parts\", \"other names\": [\"parts\", \"part\", \"section\", \"divisions\"]},\n",
    "                  {\"ENTITY\": \"Country\", \"other names\": [\"country\", \"countries\"]}],\n",
    "    \"FILTER\": [{\"ENTITY\": \"Consumer\", \"other names\": [\"consumers\", \"consumer\"], \"parent\": \"Segment\"},\n",
    "               {\"ENTITY\": \"Phone\", \"other names\": [\"phone\", \"phones\", \"mobile phones\"], \"parent\": \"Sub-Category\"},\n",
    "               {\"ENTITY\": \"Binder\", \"other names\": [\"binders\", \"binder\"], \"parent\": \"Sub-Category\"},\n",
    "               {\"ENTITY\": \"Corporate\", \"other names\": [\"corporates\", \"corporate\"], \"parent\": \"Segment\"},\n",
    "               {\"ENTITY\": \"India\", \"other names\": [\"india\"], \"parent\": \"Country\"},\n",
    "               {\"ENTITY\": \"Dubai\", \"other names\": [\"dubai\"], \"parent\": \"Country\"}],\n",
    "    \"DERIVED MEASURE\": [{\"ENTITY\": \"Ratio\",\n",
    "                         \"other names\": [\"ratio\", \"share\", \"contribution\", \"percentage\", \"proportion\", \"contributing\"]},\n",
    "                        {\"ENTITY\": \"Why\", \"other names\": [\"why\", \"cause of\", \"reason for\", \"diagnose\"]},\n",
    "                        {\"ENTITY\": \"contribution_to_growth\",\n",
    "                         \"other names\": [\"contribution to growth\", \"growth\", \"grown\"]},\n",
    "                        {\"ENTITY\": \"kda_transactional\",\n",
    "                         \"other names\": [\"kda\", \"key drivers\", \"key driver\", \"drivers\", \"driver\"]},\n",
    "                        {\"ENTITY\": \"Growth Rate\", \"other names\": [\"growth rate\", \"growth\", \"grown\"]},\n",
    "                        {\"ENTITY\": \"correlation\",\n",
    "                         \"other names\": [\"associate\", \"associated\", \"association\", \"associations\", \"correlate\",\n",
    "                                         \"correlated\",\n",
    "                                         \"correlation\", \"correlations\", \"relate\", \"related\", \"relation\", \"relations\",\n",
    "                                         \"relationship\",\n",
    "                                         \"relationships\"]}\n",
    "                        ],\n",
    "    \"DATE VARIABLE\": [\n",
    "        {\"ENTITY\": \"Order Date\", \"other names\": [\"order date\", \"date\", \"trend\", \"time\", \"when\", \"mom\", \"yoy\"]}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e3466d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_df():\n",
    "    df_final = pd.DataFrame()\n",
    "    for i in context:\n",
    "        df = pd.DataFrame(context[i])\n",
    "        df[\"Type\"]  = i\n",
    "        df_final = pd.concat([df,df_final],ignore_index=True)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float32\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"fp4\"\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "042f34aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61387c434718462998447c403c425697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# model_mistral = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=bnb_config,\n",
    "#     device_map=device_map)\n",
    "model_mistral = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "tokenizer_mistral = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e3b0ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gereneate_output_mistral(prompt):\n",
    "    #prompt = json.dumps(prompt)\n",
    "    model_input = tokenizer_mistral(prompt, return_tensors=\"pt\")#.to(\"cuda\")\n",
    "    #model.eval()\n",
    "    response = tokenizer_mistral.decode(model_mistral.generate(**model_input, max_length=4069*2)[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d12cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Setting the same prompt which is used for GPT4 open AI\n",
    "################################################################################\n",
    "\n",
    "def get_user_question_for_open_ai_prompt(question):\n",
    "    #return \"\"\"Now Based on this following \"CONTEXT\" data lookup:\\n\"\"\" + str(context) + \"\\n return a response for the question:\\n\" + question + \"\\n\"\n",
    "    return \"\\n Now detect from this question:\\n\" + question + \"\\n\"\n",
    "\n",
    "def compose_prompt_open_ai_prompt(question,base_prompt):\n",
    "    return f\"\"\"[INST]<<SYS>>\n",
    "            {base_prompt}\n",
    "            <</SYS>>\n",
    "            {get_user_question_for_open_ai_prompt(question)}\n",
    "            [/INST]\n",
    "            [_separator_]\"\"\"\n",
    "\n",
    "def post_process_open_ai_prompt_response(output):\n",
    "    mql = output.split(\"[END]\")[1].split(\"\\n\\nReasoning:\\n\\n\")[0]\n",
    "    reason = output.split(\"[END]\")[1].split(\"\\n\\nReasoning:\\n\\n\")[1]\n",
    "    print(\"*\"*50+\"MQL\"+\"*\"*50)\n",
    "    print(mql)\n",
    "    print(\"\\n\"+\"*\"*50+\"Reason\"+\"*\"*50)\n",
    "    print(reason)\n",
    "    #return mql,reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bbf608b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_model_output(output):\n",
    "    output_data = (output.split(\"[_separator_]\")[1].split(\"**Output**:\")[1])\n",
    "    lines = output_data.strip().split('\\n')\n",
    "    result = {}\n",
    "    for line in lines:\n",
    "        key, value = line.strip().split(':')\n",
    "        key = key.replace(\"- \",\"\")\n",
    "        result[key.strip()] = value.strip()\n",
    "\n",
    "    # Convert the dictionary to JSON\n",
    "    #json_string = json.dumps(result, indent=4)\n",
    "    return result\n",
    "\n",
    "def find_best_match_from_context(word, df):\n",
    "    matches = process.extractOne(word, df.values.flatten())\n",
    "    max_match, score = matches\n",
    "    index = df.values.flatten().tolist().index(max_match)\n",
    "    row_index, col_index = divmod(index, df.shape[1])\n",
    "    entity_name = df.iat[row_index, col_index]\n",
    "    return df.iloc[row_index].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08b07a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_CONSTRAINT= \"\"\"\n",
    "\n",
    "**Task**: Extract \"COMPARISON OPERATOR,\" \"COMPARISON VALUE,\" and the Targeted ENTITY\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "1. Read the input question carefully.\n",
    "2. Determine if there is a comparison intent present, such as \"greater than,\" \"less than,\" or \"equals to.\" etc, If there is a comparison intent, please proceed to extract the relevant information.\n",
    "3. Extract the \"COMPARISON OPERATOR\" based on the detected intent. Examples of \"COMPARISON OPERATORS\" include \"greater than,\" \"less than,\" and \"equals to.\"\n",
    "4. Identify and extract the \"COMPARISON VALUE\" from the sentence. This should be a numerical or scalar value.\n",
    "5. Detect the ENTITY (e.g., \"Sales\") on which these comparisons are applied. Please extract the relevant entity.\n",
    "6. Provide the following information:\n",
    "   - COMPARISON OPERATOR: (If detected else None) \n",
    "   - COMPARISON VALUE: (If detected else None)\n",
    "   - ENTITY: (If detected else None)\n",
    "\n",
    "**Input Sentence**: \"When was the first time sales of segments was 0\"\n",
    "\n",
    "**Output**:\n",
    "{\"COMPARISON OPERATOR\": \"=\",\n",
    "\"COMPARISON VALUE\": \"0\"(If detected else None)\n",
    "\"ENTITY\": \"Sales\" (If detected else None)}\n",
    "\n",
    "*Note*: If the input question does not contain any comparison intent or the entity, you can indicate that they were not detected.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "175c3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADJECTIVE_and_TONE = \"\"\"\n",
    "**Task**: Extract Adjectives, Determine Intent (TONE), Identify Entity, Return RANK ADJECTIVE, and Scalar (RANK)\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "1. Carefully read the user question to identify any adjectives present. Adjectives can include terms like \"least,\" \"highest performing,\" etc.\n",
    "2. Determine the intent (TONE) of the adjective. The intent can be either positive or negative.\n",
    "   - If the adjective intent is negative, always set RANK ADJECTIVE to \"BOTTOM\" and TONE to \"negative.\"\n",
    "   - If the adjective intent is positive, always  set RANK ADJECTIVE to \"TOP\" and TONE to \"positive.\"\n",
    "3. If the adjective implies a quantity (e.g., \"how many,\" \"top 3\"), extract that scalar value as RANK.\n",
    "4. Identify the entity (e.g., \"Segment\") on which the detected adjective should be applied.\n",
    "\n",
    "**Input Question**: \"How many top 3 segments are performing well?\"\n",
    "\n",
    "**Output**:\n",
    "- ADJECTIVE: top\n",
    "- TONE: positive\n",
    "- ENTITY: Segment\n",
    "- RANK ADJECTIVE: TOP\n",
    "- TONE: positive\n",
    "- RANK: 3\n",
    "\n",
    "**Note**: If the input question doesn't contain an adjective, doesn't imply a quantity, or if it's not clear which entity the adjective should be applied to, you can indicate that they were not detected.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1685a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_PROMPT = \"\"\"**Task: Extract Filter Information**\n",
    "\n",
    "Identify the filter entities in the user's question and set their inclusion or exclusion . For each filter entity mentioned in the question, check if it's intended to be included or excluded.\n",
    "\n",
    "Return the following attributes for each detected filter entity:\n",
    "\n",
    "EXCLUDE: Set to TRUE if the entity is intended to be excluded, FALSE otherwise.\n",
    "ENTITY: The specific filter entity identified from the question.\n",
    "\n",
    "If the question specifies exclusion for an entity, set EXCLUDE to TRUE; otherwise, set it to FALSE.\n",
    "\n",
    "**Return:**\n",
    "- EXCLUDE: TRUE or FALSE\n",
    "- ENTITY: The filter entity\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab18a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "DERIVED_MEASURE = \"\"\"Task: Extract the Derived Measure from User Queries\n",
    "\n",
    "Understand the following Derived Measure definitions:\n",
    "1. \"Ratio\": Represents a Derived Measure and can also be referred to as \"ratio,\" \"share,\" \"contribution,\" \"percentage,\" \"proportion,\" or \"contributing.\"\n",
    "2. \"Why\": Represents a Derived Measure and has synonyms such as \"why,\" \"cause of,\" \"reason for,\" and \"diagnose.\"\n",
    "3. \"Contribution to Growth\": Denotes a Derived Measure and includes synonyms like \"contribution to growth,\" \"growth,\" or \"grown.\"\n",
    "4. \"KDA Transactional\": Refers to a Derived Measure with synonyms including \"kda,\" \"key drivers,\" \"key driver,\" \"drivers,\" or \"driver.\"\n",
    "5. \"Growth Rate\": Represents a Derived Measure with synonyms like \"growth rate,\" \"growth,\" or \"grown.\"\n",
    "6. \"Correlation\": Denotes a Derived Measure and has synonyms such as \"associate,\" \"associated,\" \"association,\" \"associations,\" \"correlate,\" \"correlated,\" \"correlation,\" \"correlations,\" \"relate,\" \"related,\" \"relation,\" \"relations,\" \"relationship,\" or \"relationships.\"\n",
    "\n",
    "Your task is to examine the user's question to identify the Derived Measure concepts. If detected, find the specific targeted derived measure mentioned in the question and return it as the Derived Measure.\n",
    "\n",
    "Return:\n",
    "- Derived Measure: (if identified)\n",
    "- ENTITIES: [\"ENTITY1\", \"ENTITY2\"] (if specified in relation to the Derived Measure)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67d5e586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric_constraint model output \n",
      " : \n",
      "            {\"COMPARISON OPERATOR\": \">\",\n",
      "             \"COMPARISON VALUE\": \"100K\",\n",
      "             \"ENTITY\": \"Quantity\"}\n",
      "            \n",
      "CPU times: user 6min 58s, sys: 603 ms, total: 6min 58s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"show me quantity > 100K\"\n",
    "output=gereneate_output_mistral(compose_prompt_open_ai_prompt(question,METRIC_CONSTRAINT))\n",
    "#post_process_open_ai_prompt_response(output)\n",
    "print(f\"\"\"Metric_constraint model output \\n : {output.split(\"[_separator_]\")[1]}\"\"\")\n",
    "try:\n",
    "    metric_constraint_output = json.loads(output.split(\"[_separator_]\")[1])\n",
    "except Exception as e:\n",
    "    print(\"METRIC_CONSTRAINT model not detected any ENTITIES or Detection was not in a correct format\")\n",
    "    print(output.split(\"[_separator_]\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb7d6fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COMPARISON OPERATOR': '>', 'COMPARISON VALUE': '100K', 'ENTITY': 'Quantity'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in metric_constraint_output:\n",
    "    if i=='ENTITY':\n",
    "        return find_best_match_from_context(metric_constraint_output['ENTITY'], get_context_df())\n",
    "print(\"Metric_Constraint Model has not detected any ENTITY\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "08d2b154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ENTITY': 'correlation', 'other names': ['associate', 'associated', 'association', 'associations', 'correlate', 'correlated', 'correlation', 'correlations', 'relate', 'related', 'relation', 'relations', 'relationship', 'relationships'], 'Type': 'DERIVED MEASURE', 'parent': nan}\n"
     ]
    }
   ],
   "source": [
    "find_best_match_from_context(word, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dcc1e515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ENTITY': 'correlation',\n",
       " 'other names': ['associate',\n",
       "  'associated',\n",
       "  'association',\n",
       "  'associations',\n",
       "  'correlate',\n",
       "  'correlated',\n",
       "  'correlation',\n",
       "  'correlations',\n",
       "  'relate',\n",
       "  'related',\n",
       "  'relation',\n",
       "  'relations',\n",
       "  'relationship',\n",
       "  'relationships'],\n",
       " 'Type': 'DERIVED MEASURE',\n",
       " 'parent': nan}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_final.iloc[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db114ddb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Derived Measure: Ratio\n",
      "            ENTITIES: [\"Dubai\", \"India\"]\n",
      "            \n",
      "CPU times: user 5min 52s, sys: 947 ms, total: 5min 53s\n",
      "Wall time: 53.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Ratio of sales in Dubai vs india\"\n",
    "output=gereneate_output_mistral(compose_prompt_open_ai_prompt(question,DERIVED_MEASURE))\n",
    "#post_process_open_ai_prompt_response(output)\n",
    "print(output.split(\"[_separator_]\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a87bb35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "            **Task: Extract Filter Information**\n",
      "\n",
      "Identify the filter entities in the user's question and set their inclusion or exclusion . For each filter entity mentioned in the question, check if it's intended to be included or excluded.\n",
      "\n",
      "Return the following attributes for each detected filter entity:\n",
      "\n",
      "EXCLUDE: Set to TRUE if the entity is intended to be excluded, FALSE otherwise.\n",
      "ENTITY: The specific filter entity identified from the question.\n",
      "\n",
      "If the question specifies exclusion for an entity, set EXCLUDE to TRUE; otherwise, set it to FALSE.\n",
      "\n",
      "**Return:**\n",
      "- EXCLUDE: TRUE or FALSE\n",
      "- ENTITY: The filter entity\n",
      "            [/ _separator_]\n",
      "\n",
      "            **Filter Entities:**\n",
      "\n",
      "            - ENTITY: Phone\n",
      "            - ENTITY: Dubai\n",
      "\n",
      "            **Filter Inclusion:**\n",
      "\n",
      "            - EXCLUDE: FALSE\n",
      "            - ENTITY: Phone\n",
      "            - EXCLUDE: FALSE\n",
      "            - ENTITY: Dubai\n",
      "CPU times: user 8min 40s, sys: 808 ms, total: 8min 41s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"sales of Phone in Dubai\"\n",
    "output=gereneate_output_mistral(compose_prompt_open_ai_prompt(question,FILTER_PROMPT))\n",
    "#post_process_open_ai_prompt_response(output)\n",
    "print(output.split(\"[_separator_]\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f381d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "            **Task: Extract Filter Information**\n",
      "\n",
      "Identify the filter entities in the user's question and set their inclusion or exclusion . For each filter entity mentioned in the question, check if it's intended to be included or excluded.\n",
      "\n",
      "Return the following attributes for each detected filter entity:\n",
      "\n",
      "EXCLUDE: Set to TRUE if the entity is intended to be excluded, FALSE otherwise.\n",
      "ENTITY: The specific filter entity identified from the question.\n",
      "\n",
      "If the question specifies exclusion for an entity, set EXCLUDE to TRUE; otherwise, set it to FALSE.\n",
      "\n",
      "**Return:**\n",
      "- EXCLUDE: TRUE or FALSE\n",
      "- ENTITY: The filter entity\n",
      "            [/separator_]\n",
      "\n",
      "            **Filter Entities:**\n",
      "\n",
      "            - sales\n",
      "            - segments\n",
      "            - US\n",
      "\n",
      "            **Filter Information:**\n",
      "\n",
      "            - sales: EXCLUDE: FALSE, ENTITY: sales\n",
      "            - segments: EXCLUDE: FALSE, ENTITY: segments\n",
      "            - US: EXCLUDE: TRUE, ENTITY: US\n"
     ]
    }
   ],
   "source": [
    "question = \"sales of segements except in US\"\n",
    "output=gereneate_output_mistral(compose_prompt_open_ai_prompt(question,FILTER_PROMPT))\n",
    "#post_process_open_ai_prompt_response(output)\n",
    "print(output.split(\"[_separator_]\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9efe8be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**Task**: Extract \"COMPARISON OPERATOR,\" \"COMPARISON VALUE,\" and the Targeted ENTITY\n",
      "\n",
      "**Instructions**:\n",
      "\n",
      "1. Read the input question carefully.\n",
      "2. Determine if there is a comparison intent present, such as \"greater than,\" \"less than,\" or \"equals to.\" etc, If there is a comparison intent, please proceed to extract the relevant information.\n",
      "3. Extract the \"COMPARISON OPERATOR\" based on the detected intent. Examples of \"COMPARISON OPERATORS\" include \"greater than,\" \"less than,\" and \"equals to.\"\n",
      "4. Identify and extract the \"COMPARISON VALUE\" from the sentence. This should be a numerical or scalar value.\n",
      "5. Detect the ENTITY (e.g., \"Sales\") on which these comparisons are applied. Please extract the relevant entity.\n",
      "6. Provide the following information:\n",
      "   - COMPARISON OPERATOR: (If detected else None) \n",
      "   - COMPARISON VALUE: (If detected else None)\n",
      "   - ENTITY: (If detected else None)\n",
      "\n",
      "**Input Sentence**: \"show me quantities > 100K\"\n",
      "\n",
      "**Output**:\n",
      "- COMPARISON OPERATOR: greater than\n",
      "- COMPARISON VALUE: 100K\n",
      "- ENTITY: quantities\n",
      "CPU times: user 26.8 s, sys: 765 ms, total: 27.5 s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"show me quantities > 100K\"\n",
    "output=gereneate_output_mistral(compose_prompt_open_ai_prompt(question,METRIC_CONSTRAINT))\n",
    "#post_process_open_ai_prompt_response(output)\n",
    "print(output.split(\"[_separator_]\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "157bfe15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**Task**: Extract \"COMPARISON OPERATOR,\" \"COMPARISON VALUE,\" and the Targeted ENTITY\n",
      "\n",
      "**Instructions**:\n",
      "\n",
      "1. Read the input question carefully.\n",
      "2. Determine if there is a comparison intent present, such as \"greater than,\" \"less than,\" or \"equals to.\" etc, If there is a comparison intent, please proceed to extract the relevant information.\n",
      "3. Extract the \"COMPARISON OPERATORS\" based on the detected intent. Examples of \"COMPARISON OPERATORS\" include \"greater than,\" \"less than,\" and \"equals to.\"\n",
      "4. Identify and extract the \"COMPARISON VALUE\" from the sentence. This should be a numerical or scalar value.\n",
      "5. Detect the ENTITY (e.g., \"Sales\") on which these comparisons are applied. Please extract the relevant entity.\n",
      "6. Provide the following information:\n",
      "   - COMPARISON OPERATOR: (If detected else None) \n",
      "   - COMPARISON VALUE: (If detected else None)\n",
      "   - ENTITY: (If detected else None)\n",
      "\n",
      "**Input Sentence**: \"When is first time discount of phone is lesser than 0\"\n",
      "\n",
      "**Output**:\n",
      "- COMPARISON OPERATOR: less than\n",
      "- COMPARISON VALUE: 0\n",
      "- ENTITY: discount (phone)\n",
      "\n",
      "*Note*: If the input question does not contain any comparison intent or the entity, you can indicate that they were not detected.\n",
      "CPU times: user 29.4 s, sys: 708 ms, total: 30.1 s\n",
      "Wall time: 30.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"When is first time discount of phone is lesser than 0\"\n",
    "output=gereneate_output_mistral(compose_prompt_open_ai_prompt(question,METRIC_CONSTRAINT))\n",
    "#post_process_open_ai_prompt_response(output)\n",
    "print(output.split(\"[_separator_]\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbdd9d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**Task**: Extract Adjectives, Determine Intent (TONE), Identify Entity, Return RANK ADJECTIVE, and Scalar (RANK)\n",
      "\n",
      "**Instructions**:\n",
      "\n",
      "1. Carefully read the user question to identify any adjectives present. Adjectives can include terms like \"least,\" \"highest performing,\" etc.\n",
      "2. Determine the intent (TONE) of the adjective. The intent can be either positive or negative.\n",
      "   - If the adjective intent is negative, always set RANK ADJECTIVE to \"BOTTOM\" and TONE to \"negative.\"\n",
      "   - If the adjective intent is positive, always  set RANK ADJECTIVE to \"TOP\" and TONE to \"positive.\"\n",
      "3. If the adjective implies a quantity (e.g., \"how many,\" \"top 3\"), extract that scalar value as RANK.\n",
      "4. Identify the entity (e.g., \"Segment\") on which the detected adjective should be applied.\n",
      "\n",
      "**Input Question**: \"show me the 2 top segments basis sales\"\n",
      "\n",
      "**Output**:\n",
      "- ADJECTIVE: top\n",
      "- TONE: positive\n",
      "- ENTITY: Segment\n",
      "- RANK ADJECTIVE: TOP\n",
      "- TONE: positive\n",
      "- RANK: 2\n",
      "CPU times: user 24.9 s, sys: 744 ms, total: 25.6 s\n",
      "Wall time: 25.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"show me the 2 top segments basis sales\"\n",
    "output=gereneate_output_mistral(compose_prompt_open_ai_prompt(question,ADJECTIVE_and_TONE))\n",
    "#post_process_open_ai_prompt_response(output)\n",
    "print(output.split(\"[_separator_]\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68c4ecbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/generation/utils.py:2507: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/mosaic-ai/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)\n",
      "  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**Task**: Extract Adjectives, Determine Intent (TONE), Identify Entity, Return RANK ADJECTIVE, and Scalar (RANK)\n",
      "\n",
      "**Instructions**:\n",
      "\n",
      "1. Carefully read the user question to identify any adjectives present. Adjectives can include terms like \"top,\" \"bottom,\" \"best,\" \"worst,\" etc.\n",
      "2. Determine the intent (TONE) of the adjective. The intent can be either positive or negative.\n",
      "   - If the adjective intent is negative, always set RANK ADJECTIVE to \"BOTTOM\" and TONE to \"negative.\"\n",
      "   - If the adjective intent is positive, always  set RANK ADJECTIVE to \"TOP\" and TONE to \"positive.\"\n",
      "3. If the adjective implies a quantity (e.g., \"how many,\" \"top 3\"), extract that scalar value as RANK.\n",
      "4. Identify the entity (e.g., \"Segment\") on which the detected adjective should be applied.\n",
      "\n",
      "**Input Question**: \"top 2 segments and bottom 3 sub-category basis quantity\"\n",
      "\n",
      "**Output**:\n",
      "- ADJECTIVE: top, bottom\n",
      "- TONE: mixed (positive and negative)\n",
      "- ENTITY: Segment, sub-category\n",
      "- RANK ADJECTIVE: TOP, BOTTOM\n",
      "- TONE: positive, negative\n",
      "- RANK: 2, 3\n",
      "\n",
      "            \n",
      "CPU times: user 14min 14s, sys: 47.1 s, total: 15min 1s\n",
      "Wall time: 15min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"top 2 segments and bottom 3 sub-category basis quantity\"\n",
    "output=gereneate_output_mistral(compose_prompt_open_ai_prompt(question,ADJECTIVE_and_TONE))\n",
    "#post_process_open_ai_prompt_response(output)\n",
    "print(output.split(\"[_separator_]\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e87eebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/generation/utils.py:2507: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/mosaic-ai/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)\n",
      "  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**Task**: Extract Adjectives, Determine Intent (TONE), Identify Entity, Return RANK ADJECTIVE, and Scalar (RANK)\n",
      "\n",
      "**Instructions**:\n",
      "\n",
      "1. Carefully read the user question to identify any adjectives present. Adjectives can include terms like \"least,\" \"highest performing,\" etc.\n",
      "2. Determine the intent (TONE) of the adjective. The intent can be either positive or negative.\n",
      "   - If the adjective intent is negative, always set RANK ADJECTIVE to \"BOTTOM\" and TONE to \"negative.\"\n",
      "   - If the adjective intent is positive, always  set RANK ADJECTIVE to \"TOP\" and TONE to \"positive.\"\n",
      "3. If the adjective implies a quantity (e.g., \"how many,\" \"top 3\"), extract that scalar value as RANK.\n",
      "4. Identify the entity (e.g., \"Segment\") on which the detected adjective should be applied.\n",
      "\n",
      "**Input Question**: \"show me the 2 top segments basis sales\"\n",
      "\n",
      "**Output**:\n",
      "- ADJECTIVE: top\n",
      "- TONE: positive\n",
      "- ENTITY: Segment\n",
      "- RANK ADJECTIVE: TOP\n",
      "- TONE: positive\n",
      "- RANK: 2\n",
      "CPU times: user 18.8 s, sys: 602 ms, total: 19.4 s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"show me the 2 top segments basis sales\"\n",
    "output=gereneate_output_mistral(compose_prompt_open_ai_prompt(question,ADJECTIVE_and_TONE))\n",
    "#post_process_open_ai_prompt_response(output)\n",
    "print(output.split(\"[_separator_]\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc0f4e9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJECTIVE': 'top',\n",
       " 'TONE': 'positive',\n",
       " 'ENTITY': 'Segment',\n",
       " 'RANK ADJECTIVE': 'TOP',\n",
       " 'RANK': '2'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADJECTIVE_and_TONE_output = post_process_model_output(output)\n",
    "ADJECTIVE_and_TONE_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1601fb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DIMENSION': {'segments': [{'ENTITY': 'Segment',\n",
       "    'RANK': [{'RANK ADJECTIVE': 'top', 'RANK VALUE': '2'}]}]},\n",
       " 'MEASURE': {'sales': [{'ENTITY': 'Sales'}]}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"DIMENSION\": {\n",
    "        \"segments\": [\n",
    "            {\n",
    "                \"ENTITY\": \"Segment\",\n",
    "                \"RANK\": [\n",
    "                    {\n",
    "                        \"RANK ADJECTIVE\": \"top\",\n",
    "                        \"RANK VALUE\": \"2\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"MEASURE\": {\n",
    "        \"sales\": [\n",
    "            {\n",
    "                \"ENTITY\": \"Sales\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bcd70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b96fa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/generation/utils.py:2507: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/mosaic-ai/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)\n",
      "  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.60 GiB total capacity; 14.56 GiB already allocated; 14.94 MiB free; 14.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m, in \u001b[0;36mgereneate_output_mistral\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      3\u001b[0m model_input \u001b[38;5;241m=\u001b[39m tokenizer_mistral(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#model.eval()\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer_mistral\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mmodel_mistral\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4069\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/generation/utils.py:1606\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1590\u001b[0m         input_ids,\n\u001b[1;32m   1591\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1602\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1603\u001b[0m     )\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/generation/utils.py:2454\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2453\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2454\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2457\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2459\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2462\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py:1045\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1042\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1045\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1058\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py:932\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    925\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    926\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    927\u001b[0m         hidden_states,\n\u001b[1;32m    928\u001b[0m         attention_mask,\n\u001b[1;32m    929\u001b[0m         position_ids,\n\u001b[1;32m    930\u001b[0m     )\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 932\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    942\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py:621\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    618\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    620\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 621\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py:284\u001b[0m, in \u001b[0;36mMistralAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# repeat k/v heads if n_kv_heads < n_heads\u001b[39;00m\n\u001b[1;32m    283\u001b[0m key_states \u001b[38;5;241m=\u001b[39m repeat_kv(key_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[0;32m--> 284\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[43mrepeat_kv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_key_value_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query_states, key_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_weights\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, q_len, kv_seq_len):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py:210\u001b[0m, in \u001b[0;36mrepeat_kv\u001b[0;34m(hidden_states, n_rep)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n\u001b[1;32m    209\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, :, :]\u001b[38;5;241m.\u001b[39mexpand(batch, num_key_value_heads, n_rep, slen, head_dim)\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_key_value_heads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.60 GiB total capacity; 14.56 GiB already allocated; 14.94 MiB free; 14.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"top 2 and bottom 3 segments by sales\"\n",
    "output=gereneate_output_mistral(compose_prompt_open_ai_prompt(question,ADJECTIVE_and_TONE))\n",
    "#post_process_open_ai_prompt_response(output)\n",
    "print(output.split(\"[_separator_]\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb77851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
