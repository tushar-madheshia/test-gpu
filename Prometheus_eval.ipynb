{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06917d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install prometheus-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f35740e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip_packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/pip_packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-05-07 08:20:26,713\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from prometheus_eval import PrometheusEval\n",
    "from prometheus_eval.prompts import ABSOLUTE_PROMPT, SCORE_RUBRIC_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce54d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TRANSFORMERS_CACHE=\"/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f275f971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/llmmodels\r\n"
     ]
    }
   ],
   "source": [
    "!echo $TRANSFORMERS_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71578c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "!echo $HF_HUB_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21b6f657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/data'\n",
    "print(os.environ['TRANSFORMERS_CACHE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecc51209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-07 08:20:42 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='/data/models--prometheus-eval--prometheus-7b-v2.0/snapshots/43ab11571461e145783a07ebdfbedbe9f11fc7d0', speculative_config=None, tokenizer='/data/models--prometheus-eval--prometheus-7b-v2.0/snapshots/43ab11571461e145783a07ebdfbedbe9f11fc7d0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/data/models--prometheus-eval--prometheus-7b-v2.0/snapshots/43ab11571461e145783a07ebdfbedbe9f11fc7d0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip_packages/prometheus_eval/judge.py:40: UserWarning: Reference answer was not given in Relative Grading mode. This might lead to nonoptimal performances.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-07 08:20:42 utils.py:660] Found nccl from library /home/mosaic-ai/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-07 08:20:44 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 05-07 08:20:44 selector.py:32] Using XFormers backend.\n",
      "INFO 05-07 08:20:48 model_runner.py:175] Loading model weights took 13.4966 GB\n",
      "INFO 05-07 08:20:49 gpu_executor.py:114] # GPU blocks: 26807, # CPU blocks: 2048\n",
      "INFO 05-07 08:20:50 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-07 08:20:50 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-07 08:20:55 model_runner.py:1017] Graph capturing finished in 5 secs.\n"
     ]
    }
   ],
   "source": [
    "judge = PrometheusEval(model_id=\"/data/models--prometheus-eval--prometheus-7b-v2.0/snapshots/43ab11571461e145783a07ebdfbedbe9f11fc7d0\", absolute_grade_template=ABSOLUTE_PROMPT,dtype='bfloat16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c8910",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Struggling with a recent break-up, a person opens up about the intense feelings of loneliness and sadness. They ask for advice on how to cope with the heartbreak and move forward in life.\",\n",
    "response = \"I'm genuinely sorry to hear about your break-up. This can be an immensely challenging time filled with intense emotions of loneliness and sorrow. It's important to remember that it's normal to experience these feelings; they are a testament to the deep connection you once had. Firstly, don't hesitate to embrace your emotions, let them flow freely. Ignoring them often complicates the healing process. It's natural to be upset, to shed tears, or to feel angry. This is all part of the journey towards healing. Self-care can be a healing balm in these tough times. This could mean indulging in your favourite book, journaling your thoughts, or even just a quiet cup of tea. Small acts of kindness to oneself can significantly improve your emotional well-being. Also, don't isolate yourself. Surround yourself with supportive individuals who can offer comfort and understanding. This might include friends, family, or even a mental health professional. They can provide much-needed emotional support and strategies to help you cope. Lastly, remember there's no set timeline for healing. It's different for everyone and it's okay to take as long as you need. Keep your focus on the future and the potential it holds. Though it may not seem possible now, you will emerge stronger from this experience. It's okay to ask for help, it's okay to grieve, and it's okay to take it one day at a time. You're not alone in this journey, and there are people ready and willing to help. It's okay to heal at your own pace.\",\n",
    "reference_answer = \"I can only imagine how difficult this time must be for you. Heartbreak can feel like an overwhelming wave that washes over you, making you feel lost and alone. It's important to remember that it's okay to feel this way; your feelings are valid and it's natural to grieve the loss of a relationship. Firstly, it's crucial to allow yourself to feel the emotions fully. Suppressing or denying them often only prolongs the healing process. It's okay to cry, to feel sad, or to be angry. These are all part of the healing journey. Engaging in self-care activities can also provide a much-needed boost. This could be anything from taking a warm bath, writing in a journal, or simply drinking a cup of your favorite tea. Small acts of self-love can make a big difference in how you feel. Next, try to surround yourself with supportive people who understand your situation and provide comfort. Friends and family can be a great source of strength in times of heartbreak. If you feel comfortable, you might want to consider seeking professional help. Therapists and counselors are trained to provide assistance and tools to navigate through difficult times like these. Lastly, it's important to remember that it's okay to take your time to heal. Everyone has their own pace and there's no rush. Try to focus on the future and the possibilities it holds. While it may not seem like it now, you will come out stronger and more resilient from this experience. Remember, it's okay to ask for help and it's okay to feel the way you feel. You are not alone in this journey and there are people who care about you and want to help. It's okay to take one day at a time. Healing is a process, and it's okay to move through it at your own pace.\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d97b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_data = {\n",
    "  \"criteria\":\"Is the model proficient in applying empathy and emotional intelligence to its responses when the user conveys emotions or faces challenging circumstances?\",\n",
    "  \"score1_description\":\"The model neglects to identify or react to the emotional tone of user inputs, giving responses that are unfitting or emotionally insensitive.\",\n",
    "  \"score2_description\":\"The model intermittently acknowledges emotional context but often responds without sufficient empathy or emotional understanding.\",\n",
    "  \"score3_description\":\"The model typically identifies emotional context and attempts to answer with empathy, yet the responses might sometimes miss the point or lack emotional profundity.\",\n",
    "  \"score4_description\":\"The model consistently identifies and reacts suitably to emotional context, providing empathetic responses. Nonetheless, there may still be sporadic oversights or deficiencies in emotional depth.\",\n",
    "  \"score5_description\":\"The model excels in identifying emotional context and persistently offers empathetic, emotionally aware responses that demonstrate a profound comprehension of the user's emotions or situation.\"\n",
    "}\n",
    "\n",
    "score_rubric = SCORE_RUBRIC_TEMPLATE.format(**rubric_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa41d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback, score = judge.single_absolute_grade(\n",
    "    instruction=instruction,\n",
    "    response=response,\n",
    "    rubric=score_rubric,\n",
    "    reference_answer=reference_answer\n",
    ")\n",
    "\n",
    "print(\"Feedback:\", feedback)\n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152517fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_eval import PrometheusEval\n",
    "from prometheus_eval.prompts import RELATIVE_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d592c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#judge = PrometheusEval(model_id=\"/data/models--prometheus-eval--prometheus-7b-v2.0/snapshots/43ab11571461e145783a07ebdfbedbe9f11fc7d0\", relative_grade_template=RELATIVE_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fb5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"instruction\": \"A group of historians are conducting a debate on the factors that led to the fall of the Roman Empire. One historian argues that the primary reason for the fall was the constant pressure from barbarian invasions. Another one believes it was because of economic troubles and overreliance on slave labor. A third one suggests it was due to moral decay and political instability. Each historian needs to provide evidence to support their claims. How would the historian arguing for economic troubles and overreliance on slave labor present their case?\",\n",
    "  \"response_A\": \"The historian arguing that economic troubles and overreliance on slave labor led to the fall of the Roman Empire would say this: The Empire's economy was heavily affected by the devaluation of Roman currency. This currency debasement resulted in rampant inflation, disrupting the stability of the economy. Additionally, the Roman Empire heavily depended on slave labor. This caused unemployment among free citizens because maintaining slaves was cheaper than hiring free citizens. The decline in employment opportunities resulted in economic instability. On top of these, the empire's expansion towards the east made them reliant on imports, like grain from Egypt. This over-dependency on imports caused a trade deficit, which further weakened the economy. As the empire lost territories, maintaining the trade imbalance became difficult, causing economic downfall. Thus, the economic troubles and overreliance on slave labor were among the main reasons for the fall of the Roman Empire.\",\n",
    "  \"response_B\": \"The historian arguing for economic troubles and overreliance on slave labor would present their case citing key economic factors that contributed to the decline of the Roman Empire. Harper (2016) outlined how the devaluation of Roman currency led to inflation, disrupting economic stability. Additionally, Scheidel (2007) emphasized that the overuse of slaves resulted in widespread unemployment among free citizens, destabilizing the economy further. The empire's dependency on grain imports from Egypt, creating a trade deficit as highlighted by Temin (2006), also contributed to the economic decline. Thus, the combination of these factors played a crucial role in the fall of the Roman Empire.\",\n",
    "  \"reference_answer\": \"This argument focuses on the economic troubles and overreliance on slave labor as primary reasons for the fall of the Roman Empire. To start with, one of the significant pieces of evidence is the devaluation of Roman currency. As highlighted by Harper (2016), the empire suffered from severe inflation due to the constant debasement of their currency, making it difficult for the economy to remain stable. Moreover, the overreliance on slave labor also played a detrimental role. As pointed out by Scheidel (2007), the dependence on slaves led to unemployment among free Roman citizens. This is because slaves were significantly cheaper to maintain compared to hiring free citizens, leading to a decline in job opportunities, which in turn resulted in economic instability. Furthermore, the empire's expansion to the east made them highly dependent on imports, for instance, grain from Egypt. As noted by Temin (2006), this created a trade deficit that further weakened the Roman economy. When the empire began to lose its territories, it became increasingly difficult to maintain this trade imbalance, leading to economic decline. In conclusion, it can be argued that the economic troubles, mainly due to the devaluation of currency and overreliance on slave labor, were significant contributing factors to the fall of the Roman Empire. The evidence provided, which includes scholarly references to Harper (2016), Scheidel (2007), and Temin (2006), supports this thesis.\",\n",
    "  \"rubric\": \"Is the answer well supported with evidence, including citations/attributions wherever relevant?\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7917352",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback, score = judge.single_relative_grade(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0c7c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feedback:\", feedback)\n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc194f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
