{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82315d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32781ce429041539662446365cfb3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTBigCodeForCausalLM were not initialized from the model checkpoint at HuggingFaceH4/starchat-beta and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/starchat-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cfbbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb81f5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|system|>',\n",
       " '<|end|>',\n",
       " '<|user|>',\n",
       " 'How do I sort a list in Python?<|end|>',\n",
       " '<|assistant|>',\n",
       " \"There are multiple ways to sort a list in Python. One of the most common ways is to use the sort() method. Here's an example:\",\n",
       " '',\n",
       " '```',\n",
       " 'my_list = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]',\n",
       " 'my_list.sort()',\n",
       " 'print(my_list)',\n",
       " '```',\n",
       " '',\n",
       " 'This will sort the list in place and modify the original list. Another way toTraité',\n",
       " '',\n",
       " 'What is the time complexity of this algorithm?']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][\"generated_text\"].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4432b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We use a variant of ChatML to format each message\n",
    "# import time\n",
    "\n",
    "# t1 = time.time()\n",
    "\n",
    "# prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "# prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "# # We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "# outputs = pipe(prompt, max_new_tokens=10, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "# t2 = time.time()\n",
    "# t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1767bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f434badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "\n",
    "# examples = [\n",
    "#     \"How can I write a Python function to generate the nth Fibonacci number?\",\n",
    "#     \"How do I get the current date using shell commands? Explain how it works.\",\n",
    "#     \"What's the meaning of life?\",\n",
    "#     \"Write a function in Javascript to reverse words in a given string.\",\n",
    "#     \"Give the following data {'Name':['Tom', 'Brad', 'Kyle', 'Jerry'], 'Age':[20, 21, 19, 18], 'Height' : [6.1, 5.9, 6.0, 6.1]}. Can you plot one graph with two subplots as columns. The first is a bar graph showing the height of each person. The second is a bargraph showing the age of each person? Draw the graph in seaborn talk mode.\",\n",
    "#     \"Create a regex to extract dates from logs\",\n",
    "#     \"How to decode JSON into a typescript object\",\n",
    "#     \"Write a list into a jsonlines file and save locally\",\n",
    "# ]\n",
    "\n",
    "# def echo(message, history):\n",
    "    \n",
    "#     prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "#     prompt = prompt_template.format(query=message)\n",
    "#     # We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "#     outputs = pipe(prompt, max_new_tokens=356, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "#     return str(outputs[0][\"generated_text\"].split(\"<|assistant|>\")[1])\n",
    "    \n",
    "# demo = gr.ChatInterface(fn=echo, examples=examples, title=\"StarChat Bot\")\n",
    "# #demo.launch(share=True)\n",
    "# demo.queue(concurrency_count=3).launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d067b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /data/huggingface/cache/models--HuggingFaceH4--starchat-beta/refs/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f86073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe1ce2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3224eb7b3848ffa2d33e77588df544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTBigCodeForCausalLM were not initialized from the model checkpoint at /data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "#pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/starchat-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "pipe = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\", torch_dtype=torch.bfloat16, device_map=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8654844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3c1c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc17593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before model start\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "670.3562967777252"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "print(\"before model start\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3ec8e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before model start 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300.439350605011"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "print(\"before model start 2\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56c56792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from refractml import *\n",
    "# from refractml.constants import MLModelFlavours\n",
    "\n",
    "# # new score functions\n",
    "# from mosaic_utils.ai.score.base import ScoreBase\n",
    "# from typing import Tuple, Union, List, Any\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5934ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ScoreTemplateExample(ScoreBase):\n",
    "#     \"\"\"\n",
    "#     This Class Demonstrate How To Implements ScoreBase Interface Class And It Basic Usage.\n",
    "#     \"\"\"    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.model_loaded = None\n",
    "        \n",
    "#         import torch\n",
    "#         from transformers import pipeline\n",
    "\n",
    "#         if self.model_loaded is None:\n",
    "#             print(\"LLM model loading from data section\")\n",
    "#             self.model_loaded = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "#     def request_processing_fn(self, request) :\n",
    "#         \"\"\"\n",
    "#         Processes Request Object -> List[Input data, ..]. It could be:\n",
    "#                A List Mapping of All Value Can Be one of : \n",
    "#                    - List[ [Feature_Value1, Feature_Value2, ...], [...] ]\n",
    "#                    - List[numpy.array(), numpy.array(), ...]\n",
    "#                    - List[tf.Tensor, tf.Tensor, tf.Tensor, ...]\n",
    "#                    - List[ SingleSample, SingleSample]\n",
    "                   \n",
    "#         :return: (n_inputs, payload's)\n",
    "        \n",
    "#         Warnings:\n",
    "#         1. Do not reshape your final output for single sample here, do it in prediction.\n",
    "#            Else payloads will be invalidated for extraction at raw and extraction level.\n",
    "#         \"\"\"\n",
    "#         final_payload = []\n",
    "#         raw_payload = request.json[\"payload\"]\n",
    "#         return (1, raw_payload) \n",
    "    \n",
    "#     def pre_processing_fn(self,payload):\n",
    "#         # All preprocessing step must occur in this section\n",
    "#         # Takes Single Sample -> Returns Single Sample\n",
    "        \n",
    "#         # Not Doing Any Preprocessing Hence Returned payload\n",
    "#         print(\"payload is \", payload)\n",
    "        \n",
    "#         return payload\n",
    "\n",
    "#     def prediction_fn(self,\n",
    "#                       model: Any,\n",
    "#                       pre_processed_input \n",
    "#                       ):\n",
    "#         \"\"\"\n",
    "#                 Does the main prediction on pre_processed_input(Single Sample) using supplied model .\n",
    "\n",
    "#                 :param model: Supported Model\n",
    "#                 :param pre_processed_input: Single Preprocessed Payload\n",
    "#                 :return: Prediction Value From the model\n",
    "                \n",
    "#                 Important Notes:\n",
    "#                 - Reshape your data array.reshape(1, -1) before predictions as it contains a single sample.\n",
    "                    \n",
    "#         \"\"\"\n",
    "#         model_loaded = self.model_loaded\n",
    "#         mod = model_loaded\n",
    "#         text = pre_processed_input #this is tuple we can iterate if there is number of input\n",
    "        \n",
    "#         prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "#         prompt = prompt_template.format(query=text)\n",
    "#         # We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "#         outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "#         preds = str(outputs[0][\"generated_text\"].split(\"<|assistant|>\")[1])\n",
    "        \n",
    "        \n",
    "#         print(\"prediction is \\n: \",preds)\n",
    "    \n",
    "#         return preds\n",
    "\n",
    "#     class Meta:    \n",
    "#         # List of Callables() can be attached For Calling After AnSd Before Scoring\n",
    "#         def __init__(self):\n",
    "#             self.name = \"Pre Hooked Me !\"\n",
    "#             self.pre_call_hooks.append(self.print_)\n",
    "#         def print_(self):\n",
    "#             print(self.name)\n",
    "#         pre_call_hooks = []\n",
    "#         post_call_hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6301f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ScoreTemplateExample(ScoreBase):\n",
    "#     \"\"\"\n",
    "#     This Class Demonstrate How To Implements ScoreBase Interface Class And It Basic Usage.\n",
    "#     \"\"\"    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         import pickle\n",
    "#         from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "#         self.model_loaded = None\n",
    "#         self.tokenizer = None\n",
    "#         if self.model_loaded is None:\n",
    "#             print(\"LLM model loading from data section\")\n",
    "#             self.model_loaded = T5ForConditionalGeneration.from_pretrained(\"/data/artifacts_gpu/model/\")\n",
    "\n",
    "#         if self.tokenizer is None:\n",
    "#             print(\"tokenizer object is loading from data section\")\n",
    "#             self.tokenizer = T5Tokenizer.from_pretrained(\"/data/artifacts_gpu/tokenizer/\")\n",
    "        \n",
    "    \n",
    "#     def request_processing_fn(self, request) :\n",
    "#         \"\"\"\n",
    "#         Processes Request Object -> List[Input data, ..]. It could be:\n",
    "#                A List Mapping of All Value Can Be one of : \n",
    "#                    - List[ [Feature_Value1, Feature_Value2, ...], [...] ]\n",
    "#                    - List[numpy.array(), numpy.array(), ...]\n",
    "#                    - List[tf.Tensor, tf.Tensor, tf.Tensor, ...]\n",
    "#                    - List[ SingleSample, SingleSample]\n",
    "                   \n",
    "#         :return: (n_inputs, payload's)\n",
    "        \n",
    "#         Warnings:\n",
    "#         1. Do not reshape your final output for single sample here, do it in prediction.\n",
    "#            Else payloads will be invalidated for extraction at raw and extraction level.\n",
    "#         \"\"\"\n",
    "#         final_payload = []\n",
    "#         raw_payload = request.json[\"payload\"]\n",
    "#         return (1, raw_payload) \n",
    "    \n",
    "#     def pre_processing_fn(self,payload):\n",
    "#         # All preprocessing step must occur in this section\n",
    "#         # Takes Single Sample -> Returns Single Sample\n",
    "        \n",
    "#         # Not Doing Any Preprocessing Hence Returned payload\n",
    "#         print(\"payload is \", payload)\n",
    "        \n",
    "#         return payload\n",
    "\n",
    "#     def prediction_fn(self,\n",
    "#                       model: Any,\n",
    "#                       pre_processed_input \n",
    "#                       ):\n",
    "#         \"\"\"\n",
    "#                 Does the main prediction on pre_processed_input(Single Sample) using supplied model .\n",
    "\n",
    "#                 :param model: Supported Model\n",
    "#                 :param pre_processed_input: Single Preprocessed Payload\n",
    "#                 :return: Prediction Value From the model\n",
    "                \n",
    "#                 Important Notes:\n",
    "#                 - Reshape your data array.reshape(1, -1) before predictions as it contains a single sample.\n",
    "                    \n",
    "#         \"\"\"\n",
    "#         model_loaded = self.model_loaded\n",
    "#         mod = model_loaded\n",
    "#         text = pre_processed_input #this is tuple we can iterate if there is number of input\n",
    "#         tokenizer = self.tokenizer\n",
    "#         source = tokenizer.batch_encode_plus([text], max_length=512, pad_to_max_length=True,return_tensors='pt')\n",
    "#         source_ids = source['input_ids']\n",
    "#         source_mask = source['attention_mask']\n",
    "#         generated_ids = mod.generate(\n",
    "#             input_ids = source_ids,\n",
    "#             attention_mask = source_mask, \n",
    "#             max_length=150, \n",
    "#             num_beams=2,\n",
    "#             repetition_penalty=2.5, \n",
    "#             length_penalty=1.0, \n",
    "#             early_stopping=True\n",
    "\n",
    "#         )\n",
    "#         preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "#         print(\"prediction is \\n: \",preds)\n",
    "    \n",
    "#         return preds\n",
    "\n",
    "#     class Meta:    \n",
    "#         # List of Callables() can be attached For Calling After AnSd Before Scoring\n",
    "#         def __init__(self):\n",
    "#             self.name = \"Pre Hooked Me !\"\n",
    "#             self.pre_call_hooks.append(self.print_)\n",
    "#         def print_(self):\n",
    "#             print(self.name)\n",
    "#         pre_call_hooks = []\n",
    "#         post_call_hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34804572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: line 1: streamlit: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!streamlit run /opt/conda/lib/python3.8/site-packages/ipykernel_launcher.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abb83123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10sec.py\r\n",
      " Dashboard_App2\r\n",
      " Falcon-7B.ipynb\r\n",
      " Flant5-xl-cpu.ipynb\r\n",
      " Flant5-xl-gpu.ipynb\r\n",
      "'GAN for image Genaration.ipynb'\r\n",
      " GAN.ipynb\r\n",
      "'GPU Utilisation Check.ipynb'\r\n",
      " Image_Generation.ipynb\r\n",
      "'LLMa2 Promting'\r\n",
      " Live_Usage_queries_with_mql_processed_rev3_curated.csv\r\n",
      " LlamaCoder.ipynb\r\n",
      " Microsoft-phi1.5.ipynb\r\n",
      " Octocoder.ipynb\r\n",
      " README.md\r\n",
      "'SCB StarChat Demo.ipynb'\r\n",
      "'SCB coder-GPU.ipynb'\r\n",
      "'SCB coder.ipynb'\r\n",
      " Startcoder-tiny.ipynb\r\n",
      " T5_sum_cpu.ipynb\r\n",
      " T5_sum_gpu.ipynb\r\n",
      " Untitled.ipynb\r\n",
      " Untitled1.ipynb\r\n",
      " Untitled2.ipynb\r\n",
      " __MACOSX\r\n",
      " __pycache__\r\n",
      " chatAppSCB5\r\n",
      " dash_gunicorn\r\n",
      " dialogues.py\r\n",
      " falcon_7b_summarization.ipynb\r\n",
      " gradioContext1\r\n",
      " gradio_app\r\n",
      " gradio_app2\r\n",
      " gradio_app_new\r\n",
      " image_classification.ipynb\r\n",
      " llama-2-7b-fine-tuned-peft-v1\r\n",
      " llama-2-7b-fine-tuned-peft-v2\r\n",
      " llama2-fine-tuning-example.ipynb\r\n",
      "'llama2-fine-tuning-user-query-to-template-peft (1).ipynb'\r\n",
      " llama2-fine-tuning-user-query-to-template-peft.ipynb\r\n",
      " llm-inference-on-cpu.ipynb\r\n",
      " mistral-ft-code-generation-tryout.ipynb\r\n",
      " mistral-ft-user-query-to-mql-peft-lora.ipynb\r\n",
      " mistral-ft-user-query-to-template-peft-lora.ipynb\r\n",
      " predict_df1_0_100.csv\r\n",
      " predict_df_100_200.csv\r\n",
      " predict_df_200_300.csv\r\n",
      " predict_df_300_500.csv\r\n",
      " random.ipynb\r\n",
      " share_btn.py\r\n",
      " start_code\r\n",
      " streamlit\r\n",
      " streamlit_app\r\n",
      " streamlit_app_new\r\n",
      " template_and_user_query.csv\r\n",
      " template_and_user_query_v2.csv\r\n",
      " test_trainer\r\n",
      " thumbnail.png\r\n",
      " train_df_query_to_mql.csv\r\n",
      " trainer_tf.ipynb\r\n",
      " trainer_torch.ipynb\r\n",
      " val_df.csv\r\n",
      " val_df_prediction.csv\r\n",
      " val_df_query_to_mql.csv\r\n",
      " vit-base-beans\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8345054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nadded_tokens.json\\t\\t  \\nmodel-00004-of-00004.safetensors\\nconfig.json\\t\\t\\t  \\nmodel.safetensors.index.json\\ngeneration_config.json\\t\\t  \\nspecial_tokens_map.json\\nmerges.txt\\t\\t\\t  \\ntokenizer.json\\nmodel-00001-of-00004.safetensors  \\ntokenizer_config.json\\nmodel-00002-of-00004.safetensors  \\nvocab.json\\nmodel-00003-of-00004.safetensors\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "added_tokens.json\t\t  \n",
    "model-00004-of-00004.safetensors\n",
    "config.json\t\t\t  \n",
    "model.safetensors.index.json\n",
    "generation_config.json\t\t  \n",
    "special_tokens_map.json\n",
    "merges.txt\t\t\t  \n",
    "tokenizer.json\n",
    "model-00001-of-00004.safetensors  \n",
    "tokenizer_config.json\n",
    "model-00002-of-00004.safetensors  \n",
    "vocab.json\n",
    "model-00003-of-00004.safetensors\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fc4e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a31dcf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f89e4bb9cd4d488a2aa511dbf91752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\", torch_dtype=torch.bfloat16, device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df528d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n"
     ]
    }
   ],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "print(\"started\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79f5167e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==1.4.0\r\n",
      "accelerate==0.21.0\r\n",
      "aiohttp==3.8.5\r\n",
      "aiosignal==1.3.1\r\n",
      "alembic==1.11.1\r\n",
      "anyio==3.7.0\r\n",
      "argon2-cffi==21.3.0\r\n",
      "argon2-cffi-bindings==21.2.0\r\n",
      "asttokens==2.2.1\r\n",
      "async-generator==1.10\r\n",
      "async-timeout==4.0.2\r\n",
      "attrs==23.1.0\r\n",
      "Babel==2.12.1\r\n",
      "backcall==0.2.0\r\n",
      "beautifulsoup4==4.12.2\r\n",
      "bleach==4.1.0\r\n",
      "boltons==23.0.0\r\n",
      "bqplot==0.12.39\r\n",
      "brotlipy==0.7.0\r\n",
      "cachetools==5.3.1\r\n",
      "certifi==2023.7.22\r\n",
      "certipy==0.1.3\r\n",
      "cffi==1.15.1\r\n",
      "charset-normalizer==3.3.0\r\n",
      "cloudpickle==1.6.0\r\n",
      "conda==23.5.0\r\n",
      "conda-package-handling==2.1.0\r\n",
      "conda-package-streaming==0.8.0\r\n",
      "configparser==6.0.0\r\n",
      "cryptography==39.0.1\r\n",
      "datasets==2.14.3\r\n",
      "decorator==5.1.1\r\n",
      "defusedxml==0.7.1\r\n",
      "dill==0.3.7\r\n",
      "entrypoints==0.4\r\n",
      "evaluate==0.4.0\r\n",
      "exceptiongroup==1.1.1\r\n",
      "executing==1.2.0\r\n",
      "fastjsonschema==2.17.1\r\n",
      "filelock==3.12.4\r\n",
      "frozenlist==1.4.0\r\n",
      "fsspec==2023.9.2\r\n",
      "google-auth==2.22.0\r\n",
      "google-auth-oauthlib==1.0.0\r\n",
      "greenlet==2.0.2\r\n",
      "grpcio==1.56.2\r\n",
      "huggingface-hub==0.18.0\r\n",
      "idna==3.4\r\n",
      "importlib-metadata==6.7.0\r\n",
      "importlib-resources==5.12.0\r\n",
      "ipykernel==5.5.5\r\n",
      "ipython==8.12.2\r\n",
      "ipython-genutils==0.2.0\r\n",
      "ipywidgets==8.0.6\r\n",
      "jedi==0.18.2\r\n",
      "Jinja2==3.1.2\r\n",
      "joblib==1.3.1\r\n",
      "json5==0.9.14\r\n",
      "jsonpatch==1.32\r\n",
      "jsonpointer==2.1\r\n",
      "jsonschema==3.2.0\r\n",
      "jupyter-client==8.2.0\r\n",
      "jupyter-core==5.3.1\r\n",
      "jupyter-server==2.0.0a1\r\n",
      "jupyter-server-terminals==0.4.4\r\n",
      "jupyter-telemetry==0.1.0\r\n",
      "jupyterhub==1.5.0\r\n",
      "jupyterlab==3.2.4\r\n",
      "jupyterlab-pygments==0.2.2\r\n",
      "jupyterlab-server==2.23.0\r\n",
      "jupyterlab-widgets==3.0.7\r\n",
      "Mako==1.2.4\r\n",
      "Markdown==3.4.4\r\n",
      "MarkupSafe==2.1.3\r\n",
      "matplotlib-inline==0.1.6\r\n",
      "mistune==3.0.1\r\n",
      "mosaic-utils==1.0.2\r\n",
      "mpmath==1.2.1\r\n",
      "multidict==6.0.4\r\n",
      "multiprocess==0.70.15\r\n",
      "nbclassic==0.5.6\r\n",
      "nbclient==0.5.4\r\n",
      "nbconvert==7.6.0\r\n",
      "nbformat==5.9.0\r\n",
      "nest-asyncio==1.5.6\r\n",
      "networkx==3.0\r\n",
      "notebook==6.4.10\r\n",
      "notebook-shim==0.2.3\r\n",
      "numpy==1.24.4\r\n",
      "oauthlib==3.2.2\r\n",
      "packaging==23.2\r\n",
      "pamela==1.1.0\r\n",
      "pandas==2.0.2\r\n",
      "pandocfilters==1.5.0\r\n",
      "parso==0.8.3\r\n",
      "pexpect==4.8.0\r\n",
      "pickleshare==0.7.5\r\n",
      "Pillow==9.3.0\r\n",
      "platformdirs==3.6.0\r\n",
      "pluggy==1.0.0\r\n",
      "prometheus-client==0.17.0\r\n",
      "prompt-toolkit==3.0.38\r\n",
      "protobuf==4.23.4\r\n",
      "psutil==5.9.5\r\n",
      "ptyprocess==0.7.0\r\n",
      "pure-eval==0.2.2\r\n",
      "pyarrow==12.0.1\r\n",
      "pyasn1==0.5.0\r\n",
      "pyasn1-modules==0.3.0\r\n",
      "pycosat==0.6.4\r\n",
      "pycparser==2.21\r\n",
      "Pygments==2.15.1\r\n",
      "PyMySQL==1.1.0\r\n",
      "pyOpenSSL==23.0.0\r\n",
      "pyrsistent==0.19.3\r\n",
      "PySocks==1.7.1\r\n",
      "python-dateutil==2.8.2\r\n",
      "python-json-logger==2.0.7\r\n",
      "pytz==2023.3\r\n",
      "PyYAML==6.0.1\r\n",
      "pyzmq==25.1.0\r\n",
      "refractml==1.0.3\r\n",
      "regex==2023.10.3\r\n",
      "requests==2.31.0\r\n",
      "requests-oauthlib==1.3.1\r\n",
      "requests-toolbelt==0.9.1\r\n",
      "responses==0.18.0\r\n",
      "rsa==4.9\r\n",
      "ruamel-yaml-conda==0.15.100\r\n",
      "ruamel.yaml==0.16.9\r\n",
      "ruamel.yaml.clib==0.2.6\r\n",
      "safetensors==0.4.0\r\n",
      "scikit-learn==1.2.1\r\n",
      "scipy==1.10.1\r\n",
      "Send2Trash==1.8.2\r\n",
      "sentencepiece==0.1.99\r\n",
      "shutils==0.1.0\r\n",
      "six==1.16.0\r\n",
      "sniffio==1.3.0\r\n",
      "soupsieve==2.4.1\r\n",
      "SQLAlchemy==2.0.16\r\n",
      "stack-data==0.6.2\r\n",
      "sympy==1.11.1\r\n",
      "tensorboard==2.13.0\r\n",
      "tensorboard-data-server==0.7.1\r\n",
      "terminado==0.17.1\r\n",
      "threadpoolctl==3.2.0\r\n",
      "tinycss2==1.2.1\r\n",
      "tokenizers==0.14.1\r\n",
      "toolz==0.12.0\r\n",
      "torch==2.0.1+cpu\r\n",
      "torchaudio==2.0.2+cpu\r\n",
      "torchvision==0.15.2+cpu\r\n",
      "tornado==6.3.2\r\n",
      "tqdm==4.66.1\r\n",
      "traitlets==5.9.0\r\n",
      "traittypes==0.2.1\r\n",
      "transformers==4.34.1\r\n",
      "typing-extensions==4.8.0\r\n",
      "tzdata==2023.3\r\n",
      "urllib3==2.0.7\r\n",
      "wcwidth==0.2.6\r\n",
      "webencodings==0.5.1\r\n",
      "websocket-client==1.6.0\r\n",
      "Werkzeug==2.3.6\r\n",
      "widgetsnbextension==4.0.7\r\n",
      "xxhash==3.3.0\r\n",
      "yarl==1.9.2\r\n",
      "zipp==3.15.0\r\n",
      "zstandard==0.19.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a957e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting transformers==4.34.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/bd/f64d67df4d3b05a460f281defe830ffab6d7940b7ca98ec085e94e024781/transformers-4.34.1-py3-none-any.whl (7.7MB)\n",
      "\u001b[K     |████████████████████████████████| 7.8MB 8.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting packaging>=20.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl (53kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 88.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/6b/6600ac24725c7388255b2f5add93f91e58a5d7efaf4af244fdbcc11a541b/PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (736kB)\n",
      "\u001b[K     |████████████████████████████████| 737kB 108.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting safetensors>=0.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/cb/878e746f52e3c55312c34d89a3a80f1b2db59f5293457a75cbc99c82a27a/safetensors-0.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 99.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.15,>=0.14\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/ee/7e35fb46c728989357e6ccb96df64c4364601cfbfdd6c25ccc872e6c16a0/tokenizers-0.14.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 85.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/5d/5738903efe0ecb73e51eb44feafba32bdba2081263d40c5043568ff60faf/numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3MB 71.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.16.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl (301kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 106.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/33/67c4ed826f5227655225c3feaaecd15afb8453e827334ddae95a7fba07ac/regex-2023.10.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776kB)\n",
      "\u001b[K     |████████████████████████████████| 778kB 53.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl (78kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 99.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/5d/97afbafd9d584ff1b45fcb354a479a3609bd97f912f8f1f6c563cb1fae21/filelock-3.12.4-py3-none-any.whl\n",
      "Collecting requests\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl (62kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 99.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/d3/e1aa96437d944fbb9cc95d0316e25583886e9cd9e6adc07baad943524eda/fsspec-2023.9.2-py3-none-any.whl (173kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 102.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=3.7.4.3\n",
      "  Downloading https://files.pythonhosted.org/packages/24/21/7d397a4b7934ff4028987914ac1044d3b7d52712f30e2ac7a2ae5bc86dd0/typing_extensions-4.8.0-py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl (158kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 107.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/b2/b157855192a68541a91ba7b2bbcb91f1b4faa51f8bae38d8005c034be524/urllib3-2.0.7-py3-none-any.whl (124kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 95.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/c8/fd52271326c052f95f47ef718b018aa2bc3fd097d9bac44d7d48894c6130/charset_normalizer-3.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 118.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/34/3030de6f1370931b9dbb4dad48f6ab1015ab1d32447850b9fc94e60097be/idna-3.4-py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 107.1MB/s ta 0:00:01\n",
      "\u001b[31mERROR: tokenizers 0.14.1 has requirement huggingface_hub<0.18,>=0.16.4, but you'll have huggingface-hub 0.18.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: refractml 1.0.3 has requirement PyYAML==6.0, but you'll have pyyaml 6.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: refractml 1.0.3 has requirement urllib3==1.26.15, but you'll have urllib3 2.0.7 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jupyterlab 3.2.4 has requirement jupyter-server~=1.4, but you'll have jupyter-server 2.0.0a1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jupyterlab-server 2.23.0 has requirement jsonschema>=4.17.3, but you'll have jsonschema 3.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-auth 2.22.0 has requirement urllib3<2.0, but you'll have urllib3 2.0.7 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: packaging, pyyaml, safetensors, fsspec, typing-extensions, filelock, certifi, urllib3, charset-normalizer, idna, requests, tqdm, huggingface-hub, tokenizers, numpy, regex, transformers\n",
      "Successfully installed certifi-2023.7.22 charset-normalizer-3.3.0 filelock-3.12.4 fsspec-2023.9.2 huggingface-hub-0.18.0 idna-3.4 numpy-1.24.4 packaging-23.2 pyyaml-6.0.1 regex-2023.10.3 requests-2.31.0 safetensors-0.4.0 tokenizers-0.14.1 tqdm-4.66.1 transformers-4.34.1 typing-extensions-4.8.0 urllib3-2.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.34.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5453fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
