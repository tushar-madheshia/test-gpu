{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0c733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stock libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer, FalconModel\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/data/huggingface/cache'\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901aebd",
   "metadata": {},
   "source": [
    "## Use GPU (cuda device) if avalable to Train the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65d50db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa559bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab2deab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/huggingface/cache'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['TRANSFORMERS_CACHE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d8df9d",
   "metadata": {},
   "source": [
    "## Custom dataset for summarization\n",
    "use following constructor to pass dataframe, tokenizer, maximum length of source text and maximum length of summary length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7df538b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = summ_len\n",
    "        self.text = self.data.text\n",
    "        self.ctext = self.data.ctext\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ctext = str(self.ctext[index])\n",
    "        ctext = ' '.join(ctext.split())\n",
    "\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0992a20",
   "metadata": {},
   "source": [
    "## Creating the training function\n",
    " The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79b655ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        if _%10 == 0:\n",
    "            print({\"Training Loss\": loss.item()})\n",
    "\n",
    "        if _%500==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # xm.optimizer_step(optimizer)\n",
    "        # xm.mark_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb8112fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask, \n",
    "                max_length=150, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "                )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "            if _%100==0:\n",
    "                print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f411d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba727125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import os\n",
    "# tup = (\"hello\",\"54\",\"32cdsa\")\n",
    "# filename = '/data/artifacts/sample.pkl'\n",
    "# os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "# with open(filename, 'wb') as f:\n",
    "#     pickle.dump(tup, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be222a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/data/artifacts/sample.pkl', 'rb') as f:\n",
    "#      d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "885dc706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training  \n",
    "TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n",
    "VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n",
    "TRAIN_EPOCHS = 2       # number of epochs to train (default: 10)\n",
    "VAL_EPOCHS = 1 \n",
    "LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
    "SEED = 42               # random seed (default: 42)\n",
    "MAX_LEN = 512\n",
    "SUMMARY_LEN = 150 \n",
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "torch.manual_seed(SEED) # pytorch random seed\n",
    "np.random.seed(SEED) # numpy random seed\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "599f97c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tokenzier for encoding the text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Rocketknight1/falcon-rw-1b\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd10f0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  The Administration of Union Territory Daman an...   \n",
      "1  Malaika Arora slammed an Instagram user who tr...   \n",
      "2  The Indira Gandhi Institute of Medical Science...   \n",
      "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
      "4  Hotels in Maharashtra will train their staff t...   \n",
      "\n",
      "                                               ctext  \n",
      "0  summarize: The Daman and Diu administration on...  \n",
      "1  summarize: From her special numbers to TV?appe...  \n",
      "2  summarize: The Indira Gandhi Institute of Medi...  \n",
      "3  summarize: Lashkar-e-Taiba's Kashmir commander...  \n",
      "4  summarize: Hotels in Mumbai and other Indian c...  \n"
     ]
    }
   ],
   "source": [
    "# Importing and Pre-Processing the domain data\n",
    "# Selecting the needed columns only. \n",
    "# Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n",
    "df = pd.read_csv('/data/news_summary.csv',encoding='latin-1')\n",
    "df = df[['text','ctext']]\n",
    "df.ctext = 'summarize: ' + df.ctext\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11816f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (4514, 2)\n",
      "TRAIN Dataset: (3611, 2)\n",
      "TEST Dataset: (903, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creation of Dataset and Dataloader\n",
    "# Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
    "train_size = 0.8\n",
    "train_dataset=df.sample(frac=train_size,random_state = SEED)\n",
    "val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(val_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "900b9850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Training and Validation dataset for further creation of Dataloader\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "# Defining the parameters for creation of dataloaders\n",
    "train_params = {\n",
    "    'batch_size': TRAIN_BATCH_SIZE,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "    }\n",
    "val_params = {\n",
    "    'batch_size': VALID_BATCH_SIZE,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d483ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "val_loader = DataLoader(val_set, **val_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e912441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cd0e77708b459da20683b735d6d10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/675 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6777b211e34415ba2f1d9042e8000e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.62G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "model = FalconModel.from_pretrained(\"Rocketknight1/falcon-rw-1b\")\n",
    "model = model.to(device)\n",
    "# Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca4482d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 11M\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai  117 Sep  6 08:40 10sec.py\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai   16 Sep  6 08:40 README.md\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai  82K Sep  6 08:40 T5_sum_cpu.ipynb\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai  84K Sep  6 08:40 T5_sum_gpu.ipynb\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai 2.7M Sep  6 08:40 Untitled.ipynb\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai  97K Sep  6 08:40 falcon_7b_summarization.ipynb\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai 5.5M Sep  6 08:40 image_classification.ipynb\r\n",
      "drwxrwxrwx 3 mosaic-ai mosaic-ai 4.0K Sep  6 08:40 test_trainer\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai  25K Sep  6 08:40 trainer_tf.ipynb\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai 2.4M Sep  6 08:40 trainer_torch.ipynb\r\n",
      "drwxrwxrwx 4 mosaic-ai mosaic-ai 4.0K Sep  6 08:40 vit-base-beans\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d111734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'06/09/2023 08:46:15'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f03e685e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating Fine-Tuning for the model on our dataset\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitiating Fine-Tuning for the model on our dataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(TRAIN_EPOCHS):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, tokenizer, model, device, loader, optimizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(epoch, tokenizer, model, device, loader, optimizer):\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _,data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      4\u001b[0m         y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m      5\u001b[0m         y_ids \u001b[38;5;241m=\u001b[39m y[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     18\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext[index])\n\u001b[1;32m     19\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text\u001b[38;5;241m.\u001b[39msplit())\n\u001b[0;32m---> 21\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mctext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_max_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_encode_plus([text], max_length\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msumm_len, pad_to_max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m source_ids \u001b[38;5;241m=\u001b[39m source[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2845\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2828\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2829\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a list of sequences or a list of pairs of sequences.\u001b[39;00m\n\u001b[1;32m   2830\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[38;5;124;03m        details in `encode_plus`).\u001b[39;00m\n\u001b[1;32m   2842\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2844\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m-> 2845\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_padding_truncation_strategies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2849\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2851\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2852\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m   2855\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2856\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2871\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2872\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2482\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;66;03m# Test if we have a padding token\u001b[39;00m\n\u001b[1;32m   2481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 2482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2483\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m})`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2486\u001b[0m     )\n\u001b[1;32m   2488\u001b[0m \u001b[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2490\u001b[0m     truncation_strategy \u001b[38;5;241m!=\u001b[39m TruncationStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[1;32m   2491\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2494\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (max_length \u001b[38;5;241m%\u001b[39m pad_to_multiple_of \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2495\u001b[0m ):\n",
      "\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print('Initiating Fine-Tuning for the model on our dataset')\n",
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    train(epoch, tokenizer, model, device, training_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c550527",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = (tokenizer, model, device, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a20400fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'06/09/2023 08:46:20'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6f77982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "filename = '/data/artifacts/training_artifacts_T5_news1.pkl'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(artifacts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99009a7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary +: 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGood morning... hope the code has run successfully... the time training was started : 2023-04-02 19:19:22. And the time it ended : \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary +: 'str'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(\"Good morning... hope the code has run successfully... the time training was started : 2023-04-02 19:19:22. And the time it ended : \"+ str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89a3bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11930fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "# !ls /home/mosaic-ai/.cache/huggingface/hub/models--t5-base/snapshots/\n",
    "!mv /home/mosaic-ai/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef /home/mosaic-ai/.cache/huggingface/hub/models--t5-base/snapshots/0db7e623bcaee2daf9b859a646637ea39bf016cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd33f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_loaded = T5ForConditionalGeneration.from_pretrained(\"/data/artifacts/model/\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"/data/artifacts/tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af82c856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "filename = '/data/artifacts/training_artifacts_T5_news.pkl'\n",
    "artifacts = pickle.load(open(filename, 'rb'))\n",
    "print(type(artifacts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3c109b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(T5Tokenizer(name_or_path='t5-base', vocab_size=32100, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}),\n",
       " T5ForConditionalGeneration(\n",
       "   (shared): Embedding(32128, 768)\n",
       "   (encoder): T5Stack(\n",
       "     (embed_tokens): Embedding(32128, 768)\n",
       "     (block): ModuleList(\n",
       "       (0): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (relative_attention_bias): Embedding(32, 12)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseActDense(\n",
       "               (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "               (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (act): ReLU()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1-11): 11 x T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseActDense(\n",
       "               (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "               (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (act): ReLU()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (decoder): T5Stack(\n",
       "     (embed_tokens): Embedding(32128, 768)\n",
       "     (block): ModuleList(\n",
       "       (0): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (relative_attention_bias): Embedding(32, 12)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseActDense(\n",
       "               (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "               (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (act): ReLU()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1-11): 11 x T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseActDense(\n",
       "               (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "               (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (act): ReLU()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       " ),\n",
       " 'cpu',\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f0d3fcd99d0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5b1dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# artifacts = (tokenizer, model, device, val_loader)\n",
    "tokenizer = artifacts[0]\n",
    "model = artifacts[1]\n",
    "device = artifacts[2]\n",
    "val_loader = artifacts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f35073",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"/data/artifacts/tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e9561",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/data/artifacts/model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d11d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/Python/c3e3a6d1-48c5-4ec2-8739-9b9b511c7036/3.8/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0\n",
      "Completed 100\n"
     ]
    }
   ],
   "source": [
    "# Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
    "# Saving the dataframe as predictions.csv\n",
    "print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
    "for epoch in range(VAL_EPOCHS):\n",
    "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "    final_df.to_csv('/data/artifacts/predictions_artifacts_T5_news.csv')\n",
    "    print('Output Files generated for review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd43a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod= (tokenizer,model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79bffa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "from refractml import *\n",
    "from refractml.constants import MLModelFlavours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7813794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@scoring_func\n",
    "def score(model, request):\n",
    "    print(\"++++++++++++++++++---In Score Function---++++++++++++++++++++++\")\n",
    "    import pickle\n",
    "    from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "    model_loaded = T5ForConditionalGeneration.from_pretrained(\"/data/artifacts/model/\")\n",
    "    print(\"++++++++++++++++++---Fine-Tuned Model Loaded---++++++++++++++++++++++\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"/data/artifacts/tokenizer/\")\n",
    "    print(\"++++++++++++++++++---Tokenizer Loaded---++++++++++++++++++++++\")\n",
    "    text = request.json[\"payload\"]\n",
    "#     tokenizer = model[0]\n",
    "#     mod = model[1]\n",
    "    print(\"++++++++++++++++++---Input Text Below---++++++++++++++++++++++\")\n",
    "    print(text)\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    mod = model_loaded\n",
    "    source = tokenizer.batch_encode_plus([text], max_length=512, pad_to_max_length=True,return_tensors='pt')\n",
    "    source_ids = source['input_ids']\n",
    "    source_mask = source['attention_mask']\n",
    "    print(\"++++++++++++++++++---Input Tokenized---++++++++++++++++++++++\")\n",
    "    generated_ids = mod.generate(\n",
    "        input_ids = source_ids,\n",
    "        attention_mask = source_mask, \n",
    "        max_length=150, \n",
    "        num_beams=2,\n",
    "        repetition_penalty=2.5, \n",
    "        length_penalty=1.0, \n",
    "        early_stopping=True\n",
    "        \n",
    "    )\n",
    "    preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "    print(\"++++++++++++++++++---Prediction Done, Ouput Below---++++++++++++++++++++++\")\n",
    "    \n",
    "    print(preds)\n",
    "    \n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80cfaa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "req = requests.Request()\n",
    "req.json = {\"payload\":\"\"\"\"\n",
    "New York City Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic.\n",
    "\n",
    "During a press conference on Thursday, de Blasio said that the city will lift all restrictions on businesses, restaurants, and other venues starting on July 1st, as long as the vaccination rates continue to climb and the number of new COVID-19 cases remains low.\n",
    "\n",
    "\"This is going to be the summer of New York City,\" de Blasio said. \"We are ready to bring back the life and energy that we all know and love in this city.\"\n",
    "\n",
    "New York City was once the epicenter of the COVID-19 pandemic in the United States, with tens of thousands of cases and deaths in the early months of the outbreak. But the city has since made tremendous progress in controlling the spread of the virus, thanks in large part to an aggressive vaccination campaign that has already reached more than 6 million doses administered.\n",
    "\n",
    "The city's seven-day average positivity rate is currently at 3.18%, down from a high of 7.39% in early January. The number of hospitalizations has also dropped significantly, with just over 2,000 patients currently being treated for COVID-19, down from a peak of more than 18,000 in April 2020.\n",
    "\n",
    "The news of the city's reopening plan was met with enthusiasm from residents and business owners alike, many of whom have struggled to make ends meet during the pandemic.\n",
    "\n",
    "\"We've been waiting for this day for so long,\" said Lisa Ruiz, the owner of a small restaurant in Brooklyn. \"It's been a tough year for us, but we're ready to come back stronger than ever.\"\n",
    "\n",
    "However, some public health experts have expressed caution about the city's plans to fully reopen, noting that the virus is still present in the community and that new variants could pose a threat in the future.\n",
    "\n",
    "\"It's important that we continue to monitor the situation closely and be prepared to take action if necessary,\" said Dr. Jay Varma, the city's senior health advisor.\n",
    "\n",
    "Despite these concerns, de Blasio said that he was confident that the city's vaccination efforts and other safety measures would be enough to prevent any major outbreaks.\n",
    "\n",
    "\"We're not going to take any chances, we're going to make sure that we're following the science and the data,\" he said. \"But I believe that we can do this safely and responsibly, and we're going to make it happen.\"\n",
    "\n",
    "The announcement of New York City's reopening plan comes as other parts of the country are also beginning to lift restrictions and return to normal. California, for example, announced last month that it would fully reopen on June 15th, while other states have already lifted mask mandates and capacity limits on businesses.\n",
    "\n",
    "For many people, the news of the city's reopening is a sign of hope and optimism after a long and difficult year.\n",
    "\n",
    "\"I can't wait to go to concerts, restaurants, and all the other things that make New York City so special,\" said Paul Davis, a resident of the city. It's been a tough year, but this gives us something to look forward to.\"\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a99b6e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++---In Score Function---++++++++++++++++++++++\n",
      "++++++++++++++++++---Fine-Tuned Model Loaded---++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++---Tokenizer Loaded---++++++++++++++++++++++\n",
      "++++++++++++++++++---Input Text Below---++++++++++++++++++++++\n",
      "\"\n",
      "New York City Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic.\n",
      "\n",
      "During a press conference on Thursday, de Blasio said that the city will lift all restrictions on businesses, restaurants, and other venues starting on July 1st, as long as the vaccination rates continue to climb and the number of new COVID-19 cases remains low.\n",
      "\n",
      "\"This is going to be the summer of New York City,\" de Blasio said. \"We are ready to bring back the life and energy that we all know and love in this city.\"\n",
      "\n",
      "New York City was once the epicenter of the COVID-19 pandemic in the United States, with tens of thousands of cases and deaths in the early months of the outbreak. But the city has since made tremendous progress in controlling the spread of the virus, thanks in large part to an aggressive vaccination campaign that has already reached more than 6 million doses administered.\n",
      "\n",
      "The city's seven-day average positivity rate is currently at 3.18%, down from a high of 7.39% in early January. The number of hospitalizations has also dropped significantly, with just over 2,000 patients currently being treated for COVID-19, down from a peak of more than 18,000 in April 2020.\n",
      "\n",
      "The news of the city's reopening plan was met with enthusiasm from residents and business owners alike, many of whom have struggled to make ends meet during the pandemic.\n",
      "\n",
      "\"We've been waiting for this day for so long,\" said Lisa Ruiz, the owner of a small restaurant in Brooklyn. \"It's been a tough year for us, but we're ready to come back stronger than ever.\"\n",
      "\n",
      "However, some public health experts have expressed caution about the city's plans to fully reopen, noting that the virus is still present in the community and that new variants could pose a threat in the future.\n",
      "\n",
      "\"It's important that we continue to monitor the situation closely and be prepared to take action if necessary,\" said Dr. Jay Varma, the city's senior health advisor.\n",
      "\n",
      "Despite these concerns, de Blasio said that he was confident that the city's vaccination efforts and other safety measures would be enough to prevent any major outbreaks.\n",
      "\n",
      "\"We're not going to take any chances, we're going to make sure that we're following the science and the data,\" he said. \"But I believe that we can do this safely and responsibly, and we're going to make it happen.\"\n",
      "\n",
      "The announcement of New York City's reopening plan comes as other parts of the country are also beginning to lift restrictions and return to normal. California, for example, announced last month that it would fully reopen on June 15th, while other states have already lifted mask mandates and capacity limits on businesses.\n",
      "\n",
      "For many people, the news of the city's reopening is a sign of hope and optimism after a long and difficult year.\n",
      "\n",
      "\"I can't wait to go to concerts, restaurants, and all the other things that make New York City so special,\" said Paul Davis, a resident of the city. It's been a tough year, but this gives us something to look forward to.\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++---Input Tokenized---++++++++++++++++++++++\n",
      "++++++++++++++++++---Prediction Done, Ouput Below---++++++++++++++++++++++\n",
      "['Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic. \"We are ready to bring back the life and energy that we all know and love in this city,\" said Lisa Ruiz, the owner of a small restaurant in Brooklyn. However, public health experts have expressed caution about the city\\'s plans to fully reopen.']\n"
     ]
    }
   ],
   "source": [
    "response = score(None,req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a42ebcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic. \"We are ready to bring back the life and energy that we all know and love in this city,\" said Lisa Ruiz, the owner of a small restaurant in Brooklyn. However, public health experts have expressed caution about the city\\'s plans to fully reopen.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6758f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8babc4679a54b2bbee60b756ed1fe69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<style>.grad_1{background: #2468a4;} .grad_2{ color:white; background: #2468a4;}</s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "register_model((model,tokenizer),\n",
    "               score,\n",
    "               \"T5_Summarization_v3\",\n",
    "               \"T5_Summarization\",\n",
    "               MLModelFlavours.sklearn,\n",
    "               init_script=\"pip install SentencePiece \\\\n pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59a78188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting refractml\n",
      "  Using cached https://files.pythonhosted.org/packages/37/19/a4e26eaa2ef35252149e815df6766634185b4f8630e54c357b20cfae4ec9/refractml-1.0.2-py2.py3-none-any.whl\n",
      "Processing /home/mosaic-ai/.cache/pip/wheels/ab/d0/0e/613976a1b51b5654859e2a82ade64329859bce431e280f2a39/shutils-0.1.0-cp38-none-any.whl\n",
      "Collecting cloudpickle==1.6.0\n",
      "  Using cached https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl\n",
      "Collecting requests-toolbelt==0.9.1\n",
      "  Using cached https://files.pythonhosted.org/packages/60/ef/7681134338fc097acef8d9b2f8abe0458e4d87559c689a8c306d0957ece5/requests_toolbelt-0.9.1-py2.py3-none-any.whl\n",
      "Collecting PyYAML==6.0\n",
      "  Using cached https://files.pythonhosted.org/packages/d7/42/7ad4b6d67a16229496d4f6e74201bdbebcf4bc1e87d5a70c9297d4961bd2/PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
      "Collecting mosaic-utils==1.0.2\n",
      "  Using cached https://files.pythonhosted.org/packages/09/d7/8424b1dcaa5b1a2f824fc440aa1c4ef45e0bf6593d11b37962311614f365/mosaic_utils-1.0.2-py2.py3-none-any.whl\n",
      "Collecting pymysql\n",
      "  Using cached https://files.pythonhosted.org/packages/5b/b1/bb485db528749f07d6f11aa123e5f931f2e465a9c27945d6122bae5f7df7/PyMySQL-1.0.3-py3-none-any.whl\n",
      "Collecting configparser\n",
      "  Using cached https://files.pythonhosted.org/packages/e0/7a/9d0f52bf4923b2e410c7d6fda472c32d9b728284e89ec99074820226102f/configparser-5.3.0-py3-none-any.whl\n",
      "Collecting requests<3.0.0,>=2.0.1\n",
      "  Using cached https://files.pythonhosted.org/packages/d2/f4/274d1dbe96b41cf4e0efb70cbced278ffd61b5c7bb70338b62af94ccb25b/requests-2.28.2-py3-none-any.whl\n",
      "Collecting scikit-learn==1.2.1; python_version >= \"3.8\"\n",
      "  Using cached https://files.pythonhosted.org/packages/f0/95/0ea0a2412e33080a47ec02802210c008a7a540471581c95145f030d304b4/scikit_learn-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached https://files.pythonhosted.org/packages/74/5f/361202de730532028458b729781b8435f320e31a622c27f30e25eec80513/charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached https://files.pythonhosted.org/packages/fc/34/3030de6f1370931b9dbb4dad48f6ab1015ab1d32447850b9fc94e60097be/idna-3.4-py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached https://files.pythonhosted.org/packages/71/4c/3db2b8021bd6f2f0ceb0e088d6b2d49147671f25832fb17970e9b583d742/certifi-2022.12.7-py3-none-any.whl\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/f5/890a0baca17a61c1f92f72b81d3c31523c99bec609e60c292ea55b387ae8/urllib3-1.26.15-py2.py3-none-any.whl\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached https://files.pythonhosted.org/packages/61/cf/6e354304bcb9c6413c4e02a747b600061c21d38ba51e7e544ac7bc66aecc/threadpoolctl-3.1.0-py3-none-any.whl\n",
      "Collecting scipy>=1.3.2\n",
      "  Using cached https://files.pythonhosted.org/packages/69/f0/fb07a9548e48b687b8bf2fa81d71aba9cfc548d365046ca1c791e24db99d/scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting numpy>=1.17.3\n",
      "  Using cached https://files.pythonhosted.org/packages/8b/d9/814a619ab84d8eb0d95e08d4c723e665f1e694b5a6068ca505a61bdc3745/numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached https://files.pythonhosted.org/packages/91/d4/3b4c8e5a30604df4c7518c562d4bf0502f2fa29221459226e140cf846512/joblib-1.2.0-py3-none-any.whl\n",
      "\u001b[31mERROR: jupyterlab-server 2.19.0 has requirement jsonschema>=4.17.3, but you'll have jsonschema 3.2.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pymysql, configparser, shutils, cloudpickle, charset-normalizer, idna, certifi, urllib3, requests, requests-toolbelt, PyYAML, threadpoolctl, numpy, scipy, joblib, scikit-learn, mosaic-utils, refractml\n",
      "Successfully installed PyYAML-6.0 certifi-2022.12.7 charset-normalizer-3.1.0 cloudpickle-1.6.0 configparser-5.3.0 idna-3.4 joblib-1.2.0 mosaic-utils-1.0.2 numpy-1.24.3 pymysql-1.0.3 refractml-1.0.2 requests-2.28.2 requests-toolbelt-0.9.1 scikit-learn-1.2.1 scipy-1.10.1 shutils-0.1.0 threadpoolctl-3.1.0 urllib3-1.26.15\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/PyYAML-6.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/mosaicml already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/backports already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/requests already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy-1.10.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/cloudpickle-1.6.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/idna already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/requests-2.28.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/certifi-2022.12.7.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/refractml already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy-1.24.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/urllib3-1.26.15.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/charset_normalizer-3.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/mosaic_utils already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/refractml-1.0.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/_yaml already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/requests_toolbelt already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/yaml already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/requests_toolbelt-0.9.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/joblib-1.2.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scikit_learn.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/PyMySQL-1.0.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/mosaic_utils-1.0.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/shutils already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/threadpoolctl-3.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scikit_learn-1.2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/tests already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/charset_normalizer already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/cloudpickle already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/idna-3.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/configparser-5.3.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/joblib already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/threadpoolctl.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/sklearn already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/configparser.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/pymysql already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/shutils-0.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 23.1.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install refractml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcdadf74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'payload': \"New York City Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic. During a press conference on Thursday, de Blasio said that the city will lift all restrictions on businesses, restaurants, and other venues starting on July 1st, as long as the vaccination rates continue to climb and the number of new COVID-19 cases remains low. 'This is going to be the summer of New York City,' de Blasio said. 'We are ready to bring back the life and energy that we all know and love in this city.' New York City was once the epicenter of the COVID-19 pandemic in the United States, with tens of thousands of cases and deaths in the early months of the outbreak. But the city has since made tremendous progress in controlling the spread of the virus, thanks in large part to an aggressive vaccination campaign that has already reached more than 6 million doses administered. The city's seven-day average positivity rate is currently at 3.18%, down from a high of 7.39% in early January. The number of hospitalizations has also dropped significantly, with just over 2,000 patients currently being treated for COVID-19, down from a peak of more than 18,000 in April 2020. The news of the city's reopening plan was met with enthusiasm from residents and business owners alike, many of whom have struggled to make ends meet during the pandemic. 'We have been waiting for this day for so long,' said Lisa Ruiz, the owner of a small restaurant in Brooklyn. 'It has been a tough year for us, but we're ready to come back stronger than ever.' However, some public health experts have expressed caution about the city's plans to fully reopen, noting that the virus is still present in the community and that new variants could pose a threat in the future. 'It is important that we continue to monitor the situation closely and be prepared to take action if necessary,i said Dr. Jay Varma, the city's senior health advisor. Despite these concerns, de Blasio said that he was confident that the city's vaccination efforts and other safety measures would be enough to prevent any major outbreaks. 'We are not going to take any chances, we're going to make sure that we're following the science and the data,' he said. 'But I believe that we can do this safely and responsibly, and we're going to make it happen.' The announcement of New York City's reopening plan comes as other parts of the country are also beginning to lift restrictions and return to normal. California, for example, announced last month that it would fully reopen on June 15th, while other states have already lifted mask mandates and capacity limits on businesses. For many people, the news of the city's reopening is a sign of hope and optimism after a long and difficult year. 'I can't wait to go to concerts, restaurants, and all the other things that make New York City so special,' said Paul Davis, a resident of the city. It's been a tough year, but this gives us something to look forward to.\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"payload\":\"New York City Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic. During a press conference on Thursday, de Blasio said that the city will lift all restrictions on businesses, restaurants, and other venues starting on July 1st, as long as the vaccination rates continue to climb and the number of new COVID-19 cases remains low. 'This is going to be the summer of New York City,' de Blasio said. 'We are ready to bring back the life and energy that we all know and love in this city.' New York City was once the epicenter of the COVID-19 pandemic in the United States, with tens of thousands of cases and deaths in the early months of the outbreak. But the city has since made tremendous progress in controlling the spread of the virus, thanks in large part to an aggressive vaccination campaign that has already reached more than 6 million doses administered. The city's seven-day average positivity rate is currently at 3.18%, down from a high of 7.39% in early January. The number of hospitalizations has also dropped significantly, with just over 2,000 patients currently being treated for COVID-19, down from a peak of more than 18,000 in April 2020. The news of the city's reopening plan was met with enthusiasm from residents and business owners alike, many of whom have struggled to make ends meet during the pandemic. 'We have been waiting for this day for so long,' said Lisa Ruiz, the owner of a small restaurant in Brooklyn. 'It has been a tough year for us, but we're ready to come back stronger than ever.' However, some public health experts have expressed caution about the city's plans to fully reopen, noting that the virus is still present in the community and that new variants could pose a threat in the future. 'It is important that we continue to monitor the situation closely and be prepared to take action if necessary,i said Dr. Jay Varma, the city's senior health advisor. Despite these concerns, de Blasio said that he was confident that the city's vaccination efforts and other safety measures would be enough to prevent any major outbreaks. 'We are not going to take any chances, we're going to make sure that we're following the science and the data,' he said. 'But I believe that we can do this safely and responsibly, and we're going to make it happen.' The announcement of New York City's reopening plan comes as other parts of the country are also beginning to lift restrictions and return to normal. California, for example, announced last month that it would fully reopen on June 15th, while other states have already lifted mask mandates and capacity limits on businesses. For many people, the news of the city's reopening is a sign of hope and optimism after a long and difficult year. 'I can't wait to go to concerts, restaurants, and all the other things that make New York City so special,' said Paul Davis, a resident of the city. It's been a tough year, but this gives us something to look forward to.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36ce6fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': ['Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic. \"We are ready to bring back the life and energy that we all know and love in this city,\" said an owner of a small restaurant in Brooklyn. However, public health experts have expressed caution about the city\\'s plans to fully reopen.'],\n",
       " 'request_id': 'b38f7391-32fc-4a3b-99c6-bef4338d9671',\n",
       " 'upload_logging_data': {'end_time': '2023-04-24 05:30:00.963129',\n",
       "  'feedback': '',\n",
       "  'model_id': '636f1810-7f82-4cc5-81e4-c6ae4b0ffedc',\n",
       "  'request_id': 'b38f7391-32fc-4a3b-99c6-bef4338d9671',\n",
       "  'response_data': ['Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic. \"We are ready to bring back the life and energy that we all know and love in this city,\" said an owner of a small restaurant in Brooklyn. However, public health experts have expressed caution about the city\\'s plans to fully reopen.'],\n",
       "  'start_time': '2023-04-24 05:29:30.218434',\n",
       "  'status': 'Success',\n",
       "  'status_code': 200,\n",
       "  'version_id': '7e3864cc-852f-47de-b677-bd3251d46715'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"data\":[\"Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic. \\\"We are ready to bring back the life and energy that we all know and love in this city,\\\" said an owner of a small restaurant in Brooklyn. However, public health experts have expressed caution about the city's plans to fully reopen.\"],\"request_id\":\"b38f7391-32fc-4a3b-99c6-bef4338d9671\",\"upload_logging_data\":{\"end_time\":\"2023-04-24 05:30:00.963129\",\"feedback\":\"\",\"model_id\":\"636f1810-7f82-4cc5-81e4-c6ae4b0ffedc\",\"request_id\":\"b38f7391-32fc-4a3b-99c6-bef4338d9671\",\"response_data\":[\"Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic. \\\"We are ready to bring back the life and energy that we all know and love in this city,\\\" said an owner of a small restaurant in Brooklyn. However, public health experts have expressed caution about the city's plans to fully reopen.\"],\"start_time\":\"2023-04-24 05:29:30.218434\",\"status\":\"Success\",\"status_code\":200,\"version_id\":\"7e3864cc-852f-47de-b677-bd3251d46715\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80510fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
