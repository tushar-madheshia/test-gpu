{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6091516f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc95442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo -H pip install -q transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc01db12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall autoawq -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc12a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install autoawq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7c4a3ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !sudo -H pip install https://github.com/casper-hansen/AutoAWQ/releases/download/v0.1.7/autoawq-0.1.7+cu118-cp38-cp38-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "779984ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers==4.35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29fd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "import torch\n",
    "quant_path = '/llmmodels/quantized_model/awq_mistral'\n",
    "model_path='mistralai/Mistral-7B-v0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f89d01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = {\"zero_point\":True,\n",
    "               \"q_group_size\": 128,\n",
    "               \"w_bit\": 4,\n",
    "               \"version\": \"GEMM\"\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f853010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0cca17da3c34fb38c633751336968bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd7da91af68417e95b99f533793dd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load model\n",
    "# model = AutoAWQForCausalLM.from_pretrained(model_path, **{\"low_cpu_mem_usage\":True})\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e50621ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21ffa0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8fa7742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636aa93d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48e2dc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip_packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
      "AWQ: 100%|██████████| 32/32 [41:02<00:00, 76.96s/it]\n"
     ]
    }
   ],
   "source": [
    "#Quantize\n",
    "# dont run it again\n",
    "model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de8591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_quantized = AutoModelForCausalLM.from_pretrained(\n",
    "#     model,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "#     quantization_config=quant_config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb126257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:`quant_config.json` is being deprecated in the future in favor of quantization_config in config.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/llmmodels/quantized_model/awq_mistral/tokenizer_config.json',\n",
       " '/llmmodels/quantized_model/awq_mistral/special_tokens_map.json',\n",
       " '/llmmodels/quantized_model/awq_mistral/tokenizer.model',\n",
       " '/llmmodels/quantized_model/awq_mistral/added_tokens.json',\n",
       " '/llmmodels/quantized_model/awq_mistral/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac71427",
   "metadata": {},
   "source": [
    "### Inference on quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fc875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "quant_path = '/llmmodels/quantized_model/awq_mistral'\n",
    "model_path='mistralai/Mistral-7B-v0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ed7b2db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = quant_path\n",
    "pipe = pipeline(\"text-generation\", model=model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ace8938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken is : 20.229945421218872\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "output2 = pipe(\"I don’t know my name\",\n",
    "               max_new_tokens=500,\n",
    "               num_return_sequences=1)[0][\"generated_text\"]\n",
    "print(\"time taken is :\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "484f1d27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’t know my name. I don’'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fee82",
   "metadata": {},
   "source": [
    "### Inference on un-quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf676e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# do_sample=True,\n",
    "# top_k=50,\n",
    "# top_p=0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4a6c0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be055b0910884674829969a9fa4b37f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = model_path\n",
    "pipe = pipeline(\"text-generation\", model=model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "010b00db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken is : 50.44006586074829\n"
     ]
    }
   ],
   "source": [
    "# Using unquant model\n",
    "import time\n",
    "start = time.time()\n",
    "output1 = pipe(\"how deep learning is different from machine learning\",\n",
    "               max_new_tokens=500,\n",
    "               num_return_sequences=1)[0][\"generated_text\"]\n",
    "print(\"time taken is :\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b5b6f9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how deep learning is different from machine learning.\\n\\n## What is Machine Learning?\\n\\nMachine learning is a subset of artificial intelligence that allows computers to learn from data without being explicitly programmed. It is a method of data analysis that automates analytical model building. It is a branch of computer science that gives computers the ability to learn without being explicitly programmed.\\n\\nMachine learning algorithms build a mathematical model based on sample data, known as “training data,” in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the task.\\n\\nMachine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the task.\\n\\n## What is Deep Learning?\\n\\nDeep learning is a subset of machine learning that is concerned with algorithms inspired by the structure and function of the brain called artificial neural networks. Deep learning algorithms learn from data by using multiple layers of non-linear processing units for feature extraction and transformation.\\n\\nDeep learning is a subset of machine learning that is concerned with algorithms inspired by the structure and function of the brain called artificial neural networks. Deep learning algorithms learn from data by using multiple layers of non-linear processing units for feature extraction and transformation.\\n\\nDeep learning algorithms are used in a wide variety of applications, such as computer vision, natural language processing, and speech recognition.\\n\\n## What is the difference between Machine Learning and Deep Learning?\\n\\nThe main difference between machine learning and deep learning is that machine learning algorithms are based on linear models, while deep learning algorithms are based on non-linear models.\\n\\nMachine learning algorithms are based on linear models, while deep learning algorithms are based on non-linear models.\\n\\nMachine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the task.\\n\\nDeep learning algorithms are used in a wide variety of applications, such as computer vision, natural language processing, and speech recognition.\\n\\n## What are the benefits of Deep Learning?\\n\\nDeep learning has a number of benefits over traditional machine learning algorithms.\\n\\nDeep learning algorithms are able to learn from data that'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f139bb23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1 == output2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
