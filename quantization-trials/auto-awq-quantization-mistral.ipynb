{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc95442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo -H pip install -q transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7c4a3ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autoawq==0.1.7+cu118\n",
      "\u001b[?25l  Downloading https://github.com/casper-hansen/AutoAWQ/releases/download/v0.1.7/autoawq-0.1.7+cu118-cp38-cp38-linux_x86_64.whl (20.0MB)\n",
      "\u001b[K     |████████████████████████████████| 20.0MB 6.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /opt/conda/lib/python3.8/site-packages (from autoawq==0.1.7+cu118) (0.21.0)\n",
      "Collecting texttable\n",
      "  Downloading https://files.pythonhosted.org/packages/24/99/4772b8e00a136f3e01236de33b0efda31ee7077203ba5967fcc76da94d65/texttable-1.7.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.8/site-packages (from autoawq==0.1.7+cu118) (4.24.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.8/site-packages (from autoawq==0.1.7+cu118) (0.1.99)\n",
      "Collecting lm-eval\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/e0/05001c2e56e2f8f793189442176432ddb89a2f1f1ef00ea154d0bc00fe37/lm_eval-0.4.0.tar.gz (457kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 6.2MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tokenizers>=0.12.1 in /opt/conda/lib/python3.8/site-packages (from autoawq==0.1.7+cu118) (0.15.0)\n",
      "Collecting toml\n",
      "  Downloading https://files.pythonhosted.org/packages/44/6f/7120676b6d73228c96e17f1f794d8ab046fc910d781c8d151120c3f1569e/toml-0.10.2-py2.py3-none-any.whl\n",
      "Collecting attributedict\n",
      "  Downloading https://files.pythonhosted.org/packages/de/db/337b36e8d85a07293f5a792644fa972b53a52b13587f484c627009206697/attributedict-0.3.0-py3-none-any.whl\n",
      "Requirement already satisfied: torch>=2.0.1 in /opt/conda/lib/python3.8/site-packages (from autoawq==0.1.7+cu118) (2.0.1)\n",
      "Collecting tabulate\n",
      "  Downloading https://files.pythonhosted.org/packages/40/44/4a5f08c96eb108af5cb50b41f76142f0afa346dfa99d5296fe7202a11854/tabulate-0.9.0-py3-none-any.whl\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from autoawq==0.1.7+cu118) (0.15.2)\n",
      "Requirement already satisfied: transformers>=4.35.0 in /opt/conda/lib/python3.8/site-packages (from autoawq==0.1.7+cu118) (4.36.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from accelerate->autoawq==0.1.7+cu118) (1.21.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from accelerate->autoawq==0.1.7+cu118) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from accelerate->autoawq==0.1.7+cu118) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from accelerate->autoawq==0.1.7+cu118) (23.0)\n",
      "Collecting pybind11>=2.6.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/55/9f73c32dda93fa4f539fafa268f9504e83c489f460c380371d94296126cd/pybind11-2.11.1-py3-none-any.whl (227kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 17.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rouge-score>=0.0.4\n",
      "  Downloading https://files.pythonhosted.org/packages/e2/c5/9136736c37022a6ad27fea38f3111eb8f02fe75d067f9a985cc358653102/rouge_score-0.1.2.tar.gz\n",
      "Requirement already satisfied: scikit-learn>=0.24.1 in /opt/conda/lib/python3.8/site-packages (from lm-eval->autoawq==0.1.7+cu118) (1.3.0)\n",
      "Requirement already satisfied: zstandard in /opt/conda/lib/python3.8/site-packages (from lm-eval->autoawq==0.1.7+cu118) (0.19.0)\n",
      "Collecting numexpr\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/47/a2ede0e136a8ddc288b447c260aa035f3e75251f808aa61f6454b16dfd04/numexpr-2.8.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384kB)\n",
      "\u001b[K     |████████████████████████████████| 389kB 21.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from lm-eval->autoawq==0.1.7+cu118) (2.14.4)\n",
      "Collecting pytablewriter\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/74/b39b823ee7dba155b117634e62733a0dfdfe5aa100a553b435062cee2062/pytablewriter-1.2.0-py3-none-any.whl (111kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 22.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacrebleu>=1.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/ea/025db0a39337b63d4728a900d262c39c3029b0fe76a9876ce6297b1aa6a0/sacrebleu-2.4.0-py3-none-any.whl (106kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 37.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlitedict\n",
      "  Downloading https://files.pythonhosted.org/packages/12/9a/7620d1e9dcb02839ed6d4b14064e609cdd7a8ae1e47289aa0456796dd9ca/sqlitedict-2.1.0.tar.gz\n",
      "Requirement already satisfied: evaluate>=0.4.0 in /opt/conda/lib/python3.8/site-packages (from lm-eval->autoawq==0.1.7+cu118) (0.4.0)\n",
      "Collecting tqdm-multiprocess\n",
      "  Downloading https://files.pythonhosted.org/packages/25/7e/0d889fc6c84e3df6b69aaafe893fc77f69b3d968ac9ce574d1c62c688050/tqdm_multiprocess-0.0.11-py3-none-any.whl\n",
      "Requirement already satisfied: peft>=0.2.0 in /opt/conda/lib/python3.8/site-packages (from lm-eval->autoawq==0.1.7+cu118) (0.6.0.dev0)\n",
      "Collecting jsonlines\n",
      "  Downloading https://files.pythonhosted.org/packages/f8/62/d9ba6323b9202dd2fe166beab8a86d29465c41a0288cbe229fac60c1ab8d/jsonlines-4.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.8/site-packages (from tokenizers>=0.12.1->autoawq==0.1.7+cu118) (0.20.2)\n",
      "Collecting tox>=3.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/88/de28a027acdb3a1b070a8acdff62bd23fdc23ce32acc7ac4b92b088979a4/tox-4.11.4-py3-none-any.whl (153kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 35.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rootpath>=0.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/f9/959835686c78b7a95d8d806a97fa0be020c2deccb96de2b60659744319b9/rootpath-0.1.1-py3-none-any.whl\n",
      "Collecting deepdiff>=3.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/8f/a9d39ec15f40e8169cb134317824ee4618b864b2e4b91a9b310d3ef94729/deepdiff-6.7.1-py3-none-any.whl (76kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 26.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting codecov>=2.0.15\n",
      "  Downloading https://files.pythonhosted.org/packages/af/02/18785edcdf6266cdd6c6dc7635f1cbeefd9a5b4c3bb8aff8bd681e9dd095/codecov-2.1.13-py2.py3-none-any.whl\n",
      "Collecting colour-runner>=0.0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/82/ce3250026add1910739dcabc796571ad1d182cb47332716c8bb96ee5d624/colour_runner-0.1.1-py2.py3-none-any.whl\n",
      "Collecting coverage>=4.5.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/95/83c2429234f40f438e453554844890335c3c67e53bfd6b1e16142da893d0/coverage-7.4.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (234kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 30.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting inspecta>=0.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/03/aa/5ad8e223fa564d474b465771710b8b7b23896b59651cf115f510bcfda3ee/inspecta-0.1.3-py3-none-any.whl\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (11.4.0.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (3.1.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (10.9.0.58)\n",
      "Requirement already satisfied: triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (2.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (3.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (11.7.101)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (2.14.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (4.1.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (11.7.99)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (3.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (1.12)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision->autoawq==0.1.7+cu118) (10.0.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchvision->autoawq==0.1.7+cu118) (2.29.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.8/site-packages (from transformers>=4.35.0->autoawq==0.1.7+cu118) (0.3.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers>=4.35.0->autoawq==0.1.7+cu118) (2023.8.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers>=4.35.0->autoawq==0.1.7+cu118) (4.65.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.8/site-packages (from rouge-score>=0.0.4->lm-eval->autoawq==0.1.7+cu118) (1.4.0)\n",
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/0a/0d20d2c0f16be91b9fa32a77b76c60f9baf6eba419e5ef5deca17af9c582/nltk-3.8.1-py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 36.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from rouge-score>=0.0.4->lm-eval->autoawq==0.1.7+cu118) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.24.1->lm-eval->autoawq==0.1.7+cu118) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.24.1->lm-eval->autoawq==0.1.7+cu118) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.24.1->lm-eval->autoawq==0.1.7+cu118) (1.10.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (0.3.7)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (0.70.15)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (2.0.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (3.3.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (3.8.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (2023.6.0)\n",
      "Collecting typepy[datetime]<2,>=1.3.2\n",
      "  Downloading https://files.pythonhosted.org/packages/f1/10/0d6dc654bb4e0eca017bbaf43a315b464c888576a68a2883cd4a74bd1b6b/typepy-1.3.2-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools>=38.3.0 in /opt/conda/lib/python3.8/site-packages (from pytablewriter->lm-eval->autoawq==0.1.7+cu118) (67.8.0)\n",
      "Collecting tcolorpy<1,>=0.0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/34/d0/8a701df46bf546fd155da02934b0bb2ac6ad4186e29f4a3297e744ab259d/tcolorpy-0.1.4-py3-none-any.whl\n",
      "Collecting tabledata<2,>=1.3.1\n",
      "  Downloading https://files.pythonhosted.org/packages/06/e2/96b10ebc00d20b55967200e3d95c2137d91f58af1af672627683431c9d5c/tabledata-1.3.3-py3-none-any.whl\n",
      "Collecting pathvalidate<4,>=2.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/0c/ab/673cce13ab635fd755d206b18c0a371ef6e28ddbe25fadba9ae6c59f22a5/pathvalidate-3.2.0-py3-none-any.whl\n",
      "Collecting DataProperty<2,>=1.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/b1/3b/90ebd66ad57c588d6087e86e327436343e9cc60776a9445b79c6e80a022d/DataProperty-1.0.1-py3-none-any.whl\n",
      "Collecting mbstrdecoder<2,>=1.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/0f/726229136022b154895138bb10ba35e8435c4143f614cb5ad4d4e3fc21ec/mbstrdecoder-1.1.3-py3-none-any.whl\n",
      "Collecting colorama\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl\n",
      "Collecting portalocker\n",
      "  Downloading https://files.pythonhosted.org/packages/17/9e/87671efcca80ba6203811540ed1f9c0462c1609d2281d7b7f53cef05da3d/portalocker-2.8.2-py3-none-any.whl\n",
      "Collecting lxml\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/08/8cb09c9a0d77e7b615ace332a76e6460f4c3b0a50654c181755c619c383e/lxml-5.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0MB)\n",
      "\u001b[K     |████████████████████████████████| 8.0MB 64.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from evaluate>=0.4.0->lm-eval->autoawq==0.1.7+cu118) (0.18.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.8/site-packages (from jsonlines->lm-eval->autoawq==0.1.7+cu118) (23.1.0)\n",
      "Collecting chardet>=5.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/6f/f5fbc992a329ee4e0f288c1fe0e2ad9485ed064cac731ed2fe47dcc38cbf/chardet-5.2.0-py3-none-any.whl (199kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 106.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting platformdirs>=3.10\n",
      "  Downloading https://files.pythonhosted.org/packages/be/53/42fe5eab4a09d251a76d0043e018172db324a23fcdac70f77a551c11f618/platformdirs-4.1.0-py3-none-any.whl\n",
      "Collecting pyproject-api>=1.6.1\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/b4/39eea50542e50e93876ebc09c4349a9c9eee9f6b9c9d30f88c7dc5433db8/pyproject_api-1.6.1-py3-none-any.whl\n",
      "Collecting virtualenv>=20.24.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/22/54b1180756d2d6194bcafb7425d437c3034c4bff92129c3e1e633079e2c4/virtualenv-20.25.0-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 101.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tomli>=2.0.1; python_version < \"3.11\"\n",
      "  Downloading https://files.pythonhosted.org/packages/97/75/10a9ebee3fd790d20926a90a2547f0bf78f371b2f13aa822c759680ca7b9/tomli-2.0.1-py3-none-any.whl\n",
      "Collecting pluggy>=1.3\n",
      "  Downloading https://files.pythonhosted.org/packages/05/b8/42ed91898d4784546c5f06c60506400548db3f7a4b3fb441cba4e5c17952/pluggy-1.3.0-py3-none-any.whl\n",
      "Requirement already satisfied: cachetools>=5.3.1 in /opt/conda/lib/python3.8/site-packages (from tox>=3.0.0->attributedict->autoawq==0.1.7+cu118) (5.3.1)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl\n",
      "Collecting coloredlogs>=10.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/06/3d6badcf13db419e25b07041d9c7b4a2c331d3f4e7134445ec5df57714cd/coloredlogs-15.0.1-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 30.6MB/s eta 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hRequirement already satisfied: pygments>=2.2.0 in /opt/conda/lib/python3.8/site-packages (from rootpath>=0.1.0->attributedict->autoawq==0.1.7+cu118) (2.15.1)\n",
      "Collecting ordered-set<4.2.0,>=4.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/33/55/af02708f230eb77084a299d7b08175cff006dea4f2721074b92cdb0296c0/ordered_set-4.1.0-py3-none-any.whl\n",
      "Collecting blessings\n",
      "  Downloading https://files.pythonhosted.org/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->torch>=2.0.1->autoawq==0.1.7+cu118) (2.1.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=2.0.1->autoawq==0.1.7+cu118) (0.38.1)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=2.0.1->autoawq==0.1.7+cu118) (3.27.1)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=2.0.1->autoawq==0.1.7+cu118) (16.0.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.8/site-packages (from sympy->torch>=2.0.1->autoawq==0.1.7+cu118) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->autoawq==0.1.7+cu118) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->autoawq==0.1.7+cu118) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->autoawq==0.1.7+cu118) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->autoawq==0.1.7+cu118) (1.26.15)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk->rouge-score>=0.0.4->lm-eval->autoawq==0.1.7+cu118) (8.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (2023.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (1.4.0)\n",
      "Collecting distlib<1,>=0.3.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/41/9307e4f5f9976bc8b7fea0b66367734e8faf3ec84bc0d412d8cfabbb66cd/distlib-0.3.8-py2.py3-none-any.whl (468kB)\n",
      "\u001b[K     |████████████████████████████████| 471kB 108.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting humanfriendly>=9.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/0f/310fb31e39e2d734ccaa2c0fb981ee41f7bd5056ce9bc29b2248bd569169/humanfriendly-10.0-py2.py3-none-any.whl (86kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 48.0MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: lm-eval\n",
      "  Building wheel for lm-eval (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lm-eval: filename=lm_eval-0.4.0-cp38-none-any.whl size=994996 sha256=95b8fa3665066c6f5e9b265fd92977b8bd17c5705244f77bc0a3e04dc752ed54\n",
      "  Stored in directory: /root/.cache/pip/wheels/60/08/c8/cc3a820545d8c8990dc313dfd01c0ee280999cd9e796973e35\n",
      "Successfully built lm-eval\n",
      "Building wheels for collected packages: rouge-score, sqlitedict\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-cp38-none-any.whl size=24937 sha256=d690c8766954a6e422dbc395872d759251136ff2f5a486812eb13f63602fe184\n",
      "  Stored in directory: /root/.cache/pip/wheels/df/aa/59/74f33db3bbedf322bcaadca4a43750ea5eb523bbe742d78b25\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-cp38-none-any.whl size=16865 sha256=4bf2e70434852c52511a33fafe78e80af6c7ed63bdaa61af18fdd01ce385239b\n",
      "  Stored in directory: /root/.cache/pip/wheels/2e/10/02/9725042cf8a987b45e38bcfb106f4b0cdcc9d8968e248b93e1\n",
      "Successfully built rouge-score sqlitedict\n",
      "\u001b[31mERROR: pyproject-api 1.6.1 has requirement packaging>=23.1, but you'll have packaging 23.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tox 4.11.4 has requirement filelock>=3.12.3, but you'll have filelock 3.12.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tox 4.11.4 has requirement packaging>=23.1, but you'll have packaging 23.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: texttable, pybind11, nltk, rouge-score, numexpr, chardet, mbstrdecoder, typepy, tcolorpy, DataProperty, tabledata, pathvalidate, pytablewriter, colorama, tabulate, portalocker, lxml, sacrebleu, sqlitedict, tqdm-multiprocess, jsonlines, lm-eval, toml, platformdirs, tomli, pyproject-api, distlib, virtualenv, pluggy, tox, ordered-set, deepdiff, coverage, codecov, blessings, colour-runner, termcolor, humanfriendly, coloredlogs, rootpath, inspecta, attributedict, autoawq\n",
      "  Found existing installation: platformdirs 3.6.0\n",
      "    Uninstalling platformdirs-3.6.0:\n",
      "      Successfully uninstalled platformdirs-3.6.0\n",
      "  Found existing installation: pluggy 1.0.0\n",
      "    Uninstalling pluggy-1.0.0:\n",
      "      Successfully uninstalled pluggy-1.0.0\n",
      "Successfully installed DataProperty-1.0.1 attributedict-0.3.0 autoawq-0.1.7+cu118 blessings-1.7 chardet-5.2.0 codecov-2.1.13 colorama-0.4.6 coloredlogs-15.0.1 colour-runner-0.1.1 coverage-7.4.0 deepdiff-6.7.1 distlib-0.3.8 humanfriendly-10.0 inspecta-0.1.3 jsonlines-4.0.0 lm-eval-0.4.0 lxml-5.1.0 mbstrdecoder-1.1.3 nltk-3.8.1 numexpr-2.8.6 ordered-set-4.1.0 pathvalidate-3.2.0 platformdirs-4.1.0 pluggy-1.3.0 portalocker-2.8.2 pybind11-2.11.1 pyproject-api-1.6.1 pytablewriter-1.2.0 rootpath-0.1.1 rouge-score-0.1.2 sacrebleu-2.4.0 sqlitedict-2.1.0 tabledata-1.3.3 tabulate-0.9.0 tcolorpy-0.1.4 termcolor-2.4.0 texttable-1.7.0 toml-0.10.2 tomli-2.0.1 tox-4.11.4 tqdm-multiprocess-0.0.11 typepy-1.3.2 virtualenv-20.25.0\n"
     ]
    }
   ],
   "source": [
    "!sudo -H pip install https://github.com/casper-hansen/AutoAWQ/releases/download/v0.1.7/autoawq-0.1.7+cu118-cp38-cp38-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d8dc978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.35.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl (7.9MB)\n",
      "\u001b[K     |████████████████████████████████| 7.9MB 7.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl (62kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 32.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.19,>=0.14\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/75/56230c5c65b226e707e1adbc759c19fdf1b20bb02c0276796b132c97118a/tokenizers-0.15.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9MB 98.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/5d/5738903efe0ecb73e51eb44feafba32bdba2081263d40c5043568ff60faf/numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3MB 106.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/10/4ccc8eed80f11c082a2883d49d4090aa80c7f65704216a529f490cb089b1/regex-2023.12.25-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (777kB)\n",
      "\u001b[K     |████████████████████████████████| 778kB 97.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting packaging>=20.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl (53kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 32.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl (78kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 40.5MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/0a/aed3253a9ce63d9c90829b1d36bc44ad966499ff4f5827309099c8c9184b/huggingface_hub-0.20.2-py3-none-any.whl (330kB)\n",
      "\u001b[K     |████████████████████████████████| 337kB 110.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting safetensors>=0.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/ae/ef743263a42e21e3722845678c809e3962b6f886453431e847d520839bf0/safetensors-0.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 111.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Using cached https://files.pythonhosted.org/packages/c8/6b/6600ac24725c7388255b2f5add93f91e58a5d7efaf4af244fdbcc11a541b/PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting certifi>=2017.4.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/62/428ef076be88fa93716b576e4a01f919d25968913e817077a386fcbe4f42/certifi-2023.11.17-py3-none-any.whl (162kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 36.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/94/c31f58c7a7f470d5665935262ebd7455c7e4c7782eb525658d3dbf4b9403/urllib3-2.1.0-py3-none-any.whl (104kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 114.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/09/d82fe4a34c5f0585f9ea1df090e2a71eb9bb1e469723053e1ee9f57c16f3/charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 108.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/e7/a82b05cf63a603df6e68d59ae6a68bf5064484a0718ea5033660af4b54a9/idna-3.6-py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 38.2MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/25/fab23259a52ece5670dcb8452e1af34b89e6135ecc17cd4b54b4b479eac6/fsspec-2023.12.2-py3-none-any.whl (168kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 113.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=3.7.4.3\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/f4/6a90020cd2d93349b442bfcb657d0dc91eee65491600b2cb1d388bc98e6b/typing_extensions-4.9.0-py3-none-any.whl\n",
      "\u001b[31mERROR: refractml 1.0.3 has requirement PyYAML==6.0, but you'll have pyyaml 6.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: refractml 1.0.3 has requirement urllib3==1.26.15, but you'll have urllib3 2.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-utils 1.0.2 has requirement scikit-learn==1.2.1; python_version >= \"3.8\", but you'll have scikit-learn 1.3.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jupyterlab 3.2.4 has requirement jupyter-server~=1.4, but you'll have jupyter-server 2.0.0a1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jupyterlab-server 2.23.0 has requirement jsonschema>=4.17.3, but you'll have jsonschema 3.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-auth 2.22.0 has requirement urllib3<2.0, but you'll have urllib3 2.1.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: certifi, urllib3, charset-normalizer, idna, requests, tqdm, packaging, filelock, fsspec, typing-extensions, pyyaml, huggingface-hub, tokenizers, numpy, regex, safetensors, transformers\n",
      "Successfully installed certifi-2023.11.17 charset-normalizer-3.3.2 filelock-3.13.1 fsspec-2023.12.2 huggingface-hub-0.20.2 idna-3.6 numpy-1.24.4 packaging-23.2 pyyaml-6.0.1 regex-2023.12.25 requests-2.31.0 safetensors-0.4.1 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2 typing-extensions-4.9.0 urllib3-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29fd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bac0ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "603676d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "684c1b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia-cuda-cupti-cu11   11.7.101   \r\n",
      "nvidia-cuda-nvrtc-cu11   11.7.99    \r\n",
      "nvidia-cuda-runtime-cu11 11.7.99    \r\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb53d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffccfab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu117'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aee117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/data/mistral/query-to-mql/exp-10/nov-20/merged-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c42249",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_path = '/data/mistral/query-to-mql/exp-10/nov-20/quant-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f89d01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = {\"zero_point\":True,\n",
    "               \"q_group_size\": 128,\n",
    "               \"w_bit\": 8,\n",
    "               \"version\": \"GEMM\"\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f853010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3a9fdb9c504bddb931da9e2d96cd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load model\n",
    "# model = AutoAWQForCausalLM.from_pretrained(model_path, **{\"low_cpu_mem_usage\":True})\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path, safetensors=True, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e50621ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48e2dc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 15.60 GiB total capacity; 14.75 GiB already allocated; 120.94 MiB free; 15.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Quantize\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# dont run it again\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/awq/models/base.py:49\u001b[0m, in \u001b[0;36mBaseAWQForCausalLM.quantize\u001b[0;34m(self, tokenizer, quant_config, calib_data, split, text_column)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquantize\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, quant_config\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m     45\u001b[0m                    calib_data: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpileval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     46\u001b[0m                    split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, text_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_config: AwqConfig \u001b[38;5;241m=\u001b[39m AwqConfig\u001b[38;5;241m.\u001b[39mfrom_dict(quant_config)\n\u001b[0;32m---> 49\u001b[0m     quantizer \u001b[38;5;241m=\u001b[39m \u001b[43mAwqQuantizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw_bit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_group_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalib_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_column\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     quantizer\u001b[38;5;241m.\u001b[39mquantize()\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_quantized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/awq/quantize/quantizer.py:27\u001b[0m, in \u001b[0;36mAwqQuantizer.__init__\u001b[0;34m(self, awq_model, model, tokenizer, w_bit, group_size, version, calib_data, split, text_column)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m=\u001b[39m split\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_column \u001b[38;5;241m=\u001b[39m text_column\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/awq/quantize/quantizer.py:330\u001b[0m, in \u001b[0;36mAwqQuantizer.init_quant\u001b[0;34m(self, n_samples, seqlen)\u001b[0m\n\u001b[1;32m    328\u001b[0m modules[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m Catcher(modules[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:  \u001b[38;5;66;03m# work with early exit\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py:1009\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1006\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1022\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py:837\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    834\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, seq_length)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 837\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    840\u001b[0m     attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_flash_attn_2_enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_flash_attn_2_enabled\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    844\u001b[0m ):\n\u001b[1;32m    845\u001b[0m     is_padding_right \u001b[38;5;241m=\u001b[39m attention_mask[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m!=\u001b[39m batch_size\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 15.60 GiB total capacity; 14.75 GiB already allocated; 120.94 MiB free; 15.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#Quantize\n",
    "# dont run it again\n",
    "#model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb126257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:`quant_config.json` is being deprecated in the future in favor of quantization_config in config.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/data/quantization-trials/quant-model/tokenizer_config.json',\n",
       " '/data/quantization-trials/quant-model/special_tokens_map.json',\n",
       " '/data/quantization-trials/quant-model/vocab.json',\n",
       " '/data/quantization-trials/quant-model/merges.txt',\n",
       " '/data/quantization-trials/quant-model/added_tokens.json',\n",
       " '/data/quantization-trials/quant-model/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.save_quantized(quant_path)\n",
    "#tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac71427",
   "metadata": {},
   "source": [
    "### Inference on quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2fc875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "quant_path = '/data/quantization-trials/quant-model'\n",
    "model_path = '/data/quantization-trials/merged-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ed7b2db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = quant_path\n",
    "quant_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "def predict_from_quant(user_query):\n",
    "    _inputs = tokenizer.encode(user_query, return_tensors=\"pt\").to('cuda')\n",
    "    outputs = quant_model.generate(input_ids=_inputs, max_length= 1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    output = tokenizer.decode(outputs[0])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "574a42ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ace8938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken is : 21.147445678710938\n"
     ]
    }
   ],
   "source": [
    "# Using quant model\n",
    "start = time.time()\n",
    "output1 = predict_from_quant(\"what is art\")\n",
    "print(\"time taken is :\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "484f1d27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>what is art?\n",
      "\n",
      "Art is a way of expressing one's self. It is a way of expressing one's thoughts, feelings, and ideas. It is a way of expressing one's self through the use of various media.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to\n"
     ]
    }
   ],
   "source": [
    "print(output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fee82",
   "metadata": {},
   "source": [
    "### Inference on un-quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdf676e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba90fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/data/quantization-trials/merged-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4a6c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = model_path\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "def predict_from_normal(user_query):\n",
    "    _inputs = tokenizer.encode(user_query, return_tensors=\"pt\").to('cuda')\n",
    "    outputs = model.generate(input_ids=_inputs, max_length= 1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    output = tokenizer.decode(outputs[0])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "010b00db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken is : 23.784337282180786\n"
     ]
    }
   ],
   "source": [
    "# Using unquant model\n",
    "import time\n",
    "start = time.time()\n",
    "output2 = predict_from_normal(\"what is art\")\n",
    "print(\"time taken is :\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b5b6f9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>what is art?\n",
      "\n",
      "Art is a creative expression of the human mind. It is a way of expressing the human mind through the use of various media.\n",
      "\n",
      "The word art is derived from the Latin word artus, which means \"to make\". The word art is used in many different contexts, including:\n",
      "\n",
      "- to describe the beauty of a work of art\n",
      "\n",
      "- to describe the meaning of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of\n"
     ]
    }
   ],
   "source": [
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0949f05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
       "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f139bb23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
       "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): WQLinear_GEMM(in_features=2048, out_features=2048, bias=True, w_bit=4, group_size=512)\n",
       "            (v_proj): WQLinear_GEMM(in_features=2048, out_features=2048, bias=True, w_bit=4, group_size=512)\n",
       "            (q_proj): WQLinear_GEMM(in_features=2048, out_features=2048, bias=True, w_bit=4, group_size=512)\n",
       "            (out_proj): WQLinear_GEMM(in_features=2048, out_features=2048, bias=True, w_bit=4, group_size=512)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): WQLinear_GEMM(in_features=2048, out_features=8192, bias=True, w_bit=4, group_size=512)\n",
       "          (fc2): WQLinear_GEMM(in_features=8192, out_features=2048, bias=True, w_bit=4, group_size=512)\n",
       "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
