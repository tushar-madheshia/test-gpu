{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6091516f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc95442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo -H pip install -q transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc01db12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall autoawq -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc12a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install autoawq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7c4a3ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !sudo -H pip install https://github.com/casper-hansen/AutoAWQ/releases/download/v0.1.7/autoawq-0.1.7+cu118-cp38-cp38-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "779984ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers==4.35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29fd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "import torch\n",
    "quant_path = '/llmmodels/quantized_model/awq_mistral'\n",
    "model_path='mistralai/Mistral-7B-v0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f89d01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = {\"zero_point\":True,\n",
    "               \"q_group_size\": 128,\n",
    "               \"w_bit\": 4,\n",
    "               \"version\": \"GEMM\"\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f853010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0cca17da3c34fb38c633751336968bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd7da91af68417e95b99f533793dd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load model\n",
    "# model = AutoAWQForCausalLM.from_pretrained(model_path, **{\"low_cpu_mem_usage\":True})\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e50621ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21ffa0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8fa7742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636aa93d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48e2dc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip_packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
      "AWQ: 100%|██████████| 32/32 [41:02<00:00, 76.96s/it]\n"
     ]
    }
   ],
   "source": [
    "#Quantize\n",
    "# dont run it again\n",
    "model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de8591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_quantized = AutoModelForCausalLM.from_pretrained(\n",
    "#     model,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "#     quantization_config=quant_config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb126257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:`quant_config.json` is being deprecated in the future in favor of quantization_config in config.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/llmmodels/quantized_model/awq_mistral/tokenizer_config.json',\n",
       " '/llmmodels/quantized_model/awq_mistral/special_tokens_map.json',\n",
       " '/llmmodels/quantized_model/awq_mistral/tokenizer.model',\n",
       " '/llmmodels/quantized_model/awq_mistral/added_tokens.json',\n",
       " '/llmmodels/quantized_model/awq_mistral/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac71427",
   "metadata": {},
   "source": [
    "### Inference on quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fc875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "quant_path = '/llmmodels/quantized_model/awq_mistral'\n",
    "model_path='mistralai/Mistral-7B-v0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ed7b2db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = quant_path\n",
    "pipe = pipeline(\"text-generation\", model=model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ace8938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken is : 20.34541630744934\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "output2 = pipe(\"I love data science\",\n",
    "               max_new_tokens=500,\n",
    "               num_return_sequences=1)[0][\"generated_text\"]\n",
    "print(\"time taken is :\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "484f1d27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the long walks we used to had.\\n\\nI’m not sure if I’m ready to let go of the past. I’m not sure if I’m ready to let go of the memories. I’m not sure if I’m ready to let go of the pain. I’m not sure if I’m ready to let go of the love.\\n\\nI’m not sure if I’m ready to let go of you.\\n\\nI’m not sure if I’m ready to let go of me.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go of anything.\\n\\nI’m not sure if I’m ready to let go'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fee82",
   "metadata": {},
   "source": [
    "### Inference on un-quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf676e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# do_sample=True,\n",
    "# top_k=50,\n",
    "# top_p=0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4a6c0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be055b0910884674829969a9fa4b37f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = model_path\n",
    "pipe = pipeline(\"text-generation\", model=model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "010b00db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken is : 50.44006586074829\n"
     ]
    }
   ],
   "source": [
    "# Using unquant model\n",
    "import time\n",
    "start = time.time()\n",
    "output1 = pipe(\"how deep learning is different from machine learning\",\n",
    "               max_new_tokens=500,\n",
    "               num_return_sequences=1)[0][\"generated_text\"]\n",
    "print(\"time taken is :\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b5b6f9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how deep learning is different from machine learning.\\n\\n## What is Machine Learning?\\n\\nMachine learning is a subset of artificial intelligence that allows computers to learn from data without being explicitly programmed. It is a method of data analysis that automates analytical model building. It is a branch of computer science that gives computers the ability to learn without being explicitly programmed.\\n\\nMachine learning algorithms build a mathematical model based on sample data, known as “training data,” in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the task.\\n\\nMachine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the task.\\n\\n## What is Deep Learning?\\n\\nDeep learning is a subset of machine learning that is concerned with algorithms inspired by the structure and function of the brain called artificial neural networks. Deep learning algorithms learn from data by using multiple layers of non-linear processing units for feature extraction and transformation.\\n\\nDeep learning is a subset of machine learning that is concerned with algorithms inspired by the structure and function of the brain called artificial neural networks. Deep learning algorithms learn from data by using multiple layers of non-linear processing units for feature extraction and transformation.\\n\\nDeep learning algorithms are used in a wide variety of applications, such as computer vision, natural language processing, and speech recognition.\\n\\n## What is the difference between Machine Learning and Deep Learning?\\n\\nThe main difference between machine learning and deep learning is that machine learning algorithms are based on linear models, while deep learning algorithms are based on non-linear models.\\n\\nMachine learning algorithms are based on linear models, while deep learning algorithms are based on non-linear models.\\n\\nMachine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the task.\\n\\nDeep learning algorithms are used in a wide variety of applications, such as computer vision, natural language processing, and speech recognition.\\n\\n## What are the benefits of Deep Learning?\\n\\nDeep learning has a number of benefits over traditional machine learning algorithms.\\n\\nDeep learning algorithms are able to learn from data that'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f139bb23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1 == output2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
